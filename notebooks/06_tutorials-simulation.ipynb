{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "790f6ebe0dfe424",
   "metadata": {},
   "source": [
    "# Tutorial: Simulation\n",
    "\n",
    "# Part 1: Algorithmic Exercises on Solvers and Numerical Methods\n",
    "\n",
    "## Non-linear solver exercises\n",
    "\n",
    "These exercises are designed to reinforce the concepts covered in Chapter 4, including linear and nonlinear solvers, conditioning, scaling, Jacobians, Hessians, and Newton methods.\n",
    "\n",
    "### Exercise 1: Computing Jacobians and Hessians Analytically and Numerically\n",
    "Objective: Practice deriving and approximating Jacobians and Hessians for nonlinear functions.\n",
    "\n",
    "\n",
    "Start with the scalar function:\n",
    "\n",
    "$$f(x_1, x_2) = x_1^2 \\sin(x_2) + e^{x_1 x_2}$$\n",
    "\n",
    "1. Use SymPy to compute the analytical Jacobian (gradient) and Hessian at $(x_1, x_2) = (1, \\pi/2)$.\n",
    "2. Approximate the Jacobian and Hessian numerically using finite differences (e.g., with scipy.optimize.approx_fprime for gradient, and a similar approach for Hessian).\n",
    "3. Compare the results and discuss truncation vs. round-off errors in finite differences.\n",
    "\n",
    "Hints: For numerical Hessian, finite-difference the gradient. Vary the step size $\\epsilon$ (e.g., 1e-6 to 1e-8) to observe error trade-offs.\n",
    "\n",
    "The code below is a starting point with the symbolic and numerical coded in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84e52348d2143ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T12:23:32.843701Z",
     "start_time": "2025-11-12T12:23:32.224867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Jacobian: [[9.5562802  4.81047738]]\n",
      "Numerical Jacobian: [9.55628713 4.81047929]\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from scipy.optimize import approx_fprime\n",
    "import numpy as np\n",
    "\n",
    "# Analytical with SymPy\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "f_analytical = x1**2 * sp.sin(x2) + sp.exp(x1 * x2)\n",
    "\n",
    "# Numerical function\n",
    "def f_num(x):\n",
    "    return x[0]**2 * np.sin(x[1]) + np.exp(x[0] * x[1])\n",
    "\n",
    "#\n",
    "print(f\"Expected result = [9.5562802  4.81047738]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bebd004a1aa0f1",
   "metadata": {},
   "source": [
    "### Excercise 2: Vector-valued functions\n",
    "\n",
    "Next, repeat the exercise for the vector function:\n",
    "\n",
    "$$F(x_1, x_2) = [ x_1^2 \\sin(x_2) + e^{x_1 x_2}, x_2^2 \\sin(x_1) + e^{x_1 x_2}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac03cd0c01a3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical with SymPy\n",
    "x1, x2 = sp.symbols('x1 x2')\n",
    "f_analytical = [x1**2 * sp.sin(x2) + sp.exp(x1 * x2),\n",
    "                x2**2 * sp.sin(x1) + sp.exp(x1 * x2)]\n",
    "\n",
    "# Numerical function\n",
    "def f_num(x):\n",
    "    return [x[0]**2 * np.sin(x[1]) + np.exp(x[0] * x[1]),\n",
    "            x[1]**2 * np.sin(x[0]) + np.exp(x[0] * x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cacc2724836142",
   "metadata": {},
   "source": [
    "### Exercise 3: Implementing a Single Gauss-Newton Iteration\n",
    "#### 3.1) Gradient descent step\n",
    "\n",
    "The purpose of this exercise is to gain intuition for the Gauss-Newton method through manual implementation and visualization.\n",
    "\n",
    "For the nonlinear system $\\mathbf{F}(\\mathbf{x}) = \\begin{pmatrix} x_1 + x_2 - 3 \\\\ x_1^2 + x_2^2 - 5 \\end{pmatrix} = \\mathbf{0}$:\n",
    "\n",
    "1. Start from $\\mathbf{x}_0 = (1, 1.5)$.\n",
    "2. Compute the Jacobian $\\mathbf{J}(\\mathbf{x}_0)$ analytically or numerically.\n",
    "3. Solve for the search direction $\\mathbf{p}$ using the Gauss-Newton normal equations.\n",
    "4. Update $\\mathbf{x}_1 = \\mathbf{x}_0 + \\mathbf{p}$ and evaluate $\\|\\mathbf{F}(\\mathbf{x}_1)\\|$.\n",
    "5. Visualize the contours of $S(\\mathbf{x}) = \\frac{1}{2} \\|\\mathbf{F}(\\mathbf{x})\\|^2$ and plot the step.\n",
    "\n",
    "Hints: Use Matplotlib for contour plots. Compare with scipy.optimize.least_squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025582dcfd48fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of F(x) = 0:\n",
    "def F(x):\n",
    "    return np.array([x[0] + x[1] - 3,\n",
    "                     x[0]**2 + x[1]**2 - 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf29141e35731aa",
   "metadata": {},
   "source": [
    "#### 3.2) Improving Gauss-Newton step with line search\n",
    "\n",
    "The above equation assumed that $\\alpha_k = 1$ in the line search:\n",
    "\n",
    "$$\\mathbf{x_{k+1}} = \\mathbf{x_k} + \\alpha_k \\mathbf{p_k}$$\n",
    "\n",
    "When we have knowledge of the Hessian $f''(x) = \\nabla^2 f(x) = H_f(x)\\in \\mathbb{R}^{d\\times d}$, we can obtain the classical Newton step ($iff$ the Hessian is invertible)):\n",
    "\n",
    "$$x_{k+1} = x_k - [f''(x_k)]^{-1} f'(x_k)$$\n",
    "\n",
    "An operator that is closely related to the Hessian is the Laplacian operator, defined as the divergence of the gradient of a function. In multiple dimensions, the Laplacian is given by:\n",
    "\n",
    "$$ \\nabla \\cdot \\nabla  f = \\sum_{i=1}^{d} \\frac{\\partial^2 f}{\\partial x_i^2} $$\n",
    "\n",
    "Because the Laplacian doesn't need to be inverted, it can be used to approximate the Newton step and is more suitable to higher dimensional problems with a large number of entries in the Hessian matrix. Implement the following two steps to improve the Gauss-Newton method:\n",
    "\n",
    "##### 3.2.1) Pseudo-Newton step\n",
    "\n",
    "Use the Laplacian operator to approximate value of $\\alpha_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec38f0969ac728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b9f3ea639d5320e",
   "metadata": {},
   "source": [
    "##### 3.2.2) Full Newton step\n",
    "\n",
    "Implement the full matrix multiplication to $H^-1 \\nabla f$ to compute the Newton step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6782774d2a3784a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5c006f2880411d6",
   "metadata": {},
   "source": [
    "##### 3.2.3) Compare the two methods\n",
    "\n",
    "Compare the performance of 3.2.1 and 3.2.2 on a contour plot, a good metric for this performance is to measure the reduction in the residual $\\|\\mathbf{F}(\\mathbf{x})\\|$ after one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8290f823a2bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18de8ca2e27b28f3",
   "metadata": {},
   "source": [
    "### 3.3) A full solver\n",
    "\n",
    "You now have the building blocks to implement a full Gauss-Newton solver. Implement the iterative procedure:\n",
    "1. Initialize $\\mathbf{x}_0$.\n",
    "2. While not converged:\n",
    "   - Compute $\\mathbf{F}(\\mathbf{x}_k)$ and $\\mathbf{J}(\\mathbf{x}_k)$.\n",
    "   - Solve for $\\mathbf{p}_k$.\n",
    "   - Perform line search to find $\\alpha_k$.\n",
    "   - Update $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n",
    "\n",
    "\n",
    "Choose any of the methods you developed in 3.2 and compare the convergence behavior (number of iterations, final residual) with scipy.optimize.fsolve using the same initial guess.\n",
    "\n",
    "Below is a skeleton to get you started, but feel free to modify as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac18a043695fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_gauss_newton(F, x0, tol=1e-6, max_iter=100):\n",
    "    xk = x0\n",
    "    for k in range(max_iter):\n",
    "        # Compute F and J\n",
    "        # Solve for pk\n",
    "        #\n",
    "        # Update x_k+1\n",
    "        xk = xk + alpha_k * pk\n",
    "        if np.linalg.norm(Fk) < tol:\n",
    "            break\n",
    "    return xk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682a645ec8005c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77e163b7a8a9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cabd8e89ae619457",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Solver Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcd1a395bc53ae",
   "metadata": {},
   "source": [
    "## Exercise 4: Analyzing Matrix Conditioning and Scaling\n",
    "\n",
    "The purpose of this exercise is to understand the impact of matrix conditioning on solution stability and explore scaling techniques to improve numerical accuracy.\n",
    "\n",
    "Problem: Consider the linear system $\\mathbf{Ax} = \\mathbf{b}$ where:\n",
    "$$\\mathbf{A} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.0001 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 2 \\\\ 2.0001 \\end{pmatrix}.$$\n",
    "\n",
    "Compute the condition number of $\\mathbf{A}$ using np.linalg.cond.\n",
    "Solve for $\\mathbf{x}$ and perturb $\\mathbf{b}$ by a small amount (e.g., add 0.001 to each element). Observe the change in $\\mathbf{x}$.\n",
    "Scale the rows of $\\mathbf{A}$ and $\\mathbf{b}$ to have unit norm (e.g., divide each row by its Euclidean norm) and repeat the process. Compare the condition numbers and solution sensitivities.\n",
    "\n",
    "Hints: Use np.linalg.solve for solving. Discuss why scaling helps in ill-conditioned systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38adb73980a9d40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:34:24.172648Z",
     "start_time": "2025-11-12T15:34:24.170201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number: 40002.000074915224\n",
      "Relative change in x: 0.0007071067811864696\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define A and b\n",
    "A = np.array([[1, 1], [1, 1.0001]])\n",
    "b = np.array([2, 2.0001])\n",
    "\n",
    "# Compute condition number\n",
    "cond = np.linalg.cond(A)\n",
    "print(f\"Condition number: {cond}\")\n",
    "\n",
    "# Solve original system\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "# Perturb b and solve again\n",
    "b_perturbed = b + 0.001\n",
    "x_perturbed = np.linalg.solve(A, b_perturbed)\n",
    "print(f\"Relative change in x: {np.linalg.norm(x - x_perturbed) / np.linalg.norm(x)}\")\n",
    "\n",
    "# Scaling: Normalize rows\n",
    "row_norms = np.linalg.norm(A, axis=1)\n",
    "A_scaled = A / row_norms[:, np.newaxis]\n",
    "b_scaled = b / row_norms\n",
    "\n",
    "# Repeat condition and solves..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a976ca21de9a0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a716931f198263d3",
   "metadata": {},
   "source": [
    "## Excercise 6: Implementing and Comparing Gaussian Elimination vs. LU Decomposition\n",
    "\n",
    "In the excercise we want to demonstrate the relationship between Gaussian elimination (which you might remember from undergrad linear algebra classes) and LU decomposition, implement both from scratch, and compare their efficiency and stability for solving linear systems.\n",
    "\n",
    "Consider the linear system $\\mathbf{Ax} = \\mathbf{b}$ with:\n",
    "\n",
    "$$\\mathbf{A} = \\begin{pmatrix} 2 & 1 & 1 \\\\ 4 & -6 & 0 \\\\ -2 & 7 & 2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 5 \\\\ -2 \\\\ 9 \\end{pmatrix}.$$\n",
    "\n",
    "1.Implement Gaussian elimination without pivoting to solve for $\\mathbf{x}$. Track the elimination steps and compute the effective upper triangular matrix.\n",
    "2. Implement LU decomposition without pivoting (i.e., factor $\\mathbf{A} = \\mathbf{LU}$, where $\\mathbf{L}$ has 1s on the diagonal).\n",
    "3. Solve the system using the LU factors (forward substitution for $\\mathbf{Ly} = \\mathbf{b}$, then back substitution for $\\mathbf{Ux} = \\mathbf{y}$).\n",
    "4. Compare the results from both methods. Then, test with multiple $\\mathbf{b}$ vectors (e.g., add $\\mathbf{b}_2 = [1, 2, 3]^\\top$) and measure computation time using %timeit in Jupyter.\n",
    "5. Introduce a small perturbation to $\\mathbf{A}$ (e.g., add 1e-10 to one element) and discuss stability differences. Optionally, add partial pivoting to both implementations and recompare.\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. In Gaussian elimination, augment $\\mathbf{A}$ with $\\mathbf{b}$ and perform row operations.\n",
    "1. For LU, store multipliers in $\\mathbf{L}$ below the diagonal during elimination.\n",
    "1. Use NumPy for array operations but avoid built-in solvers like np.linalg.solve—implement the algorithms manually for learning.\n",
    "1. Reference SciPy's scipy.linalg.lu_factor for validation, but explain why it's more robust.\n",
    "\n",
    "Both methods should yield $\\mathbf{x} \\approx [1, 2, 2]^\\top$. LU is more efficient for multiple $\\mathbf{b}$ since factorization is done once. Without pivoting, both can fail on singular/ill-conditioned matrices—discuss how partial pivoting (e.g., swapping rows) improves stability, mirroring real-world implementations like `scipy.linalg.lu`.\n",
    "\n",
    "The full algorithms are provided below to get you started:\n",
    "\n",
    "#### Gaussian Elimination Algorithm\n",
    "Gaussian elimination transforms the augmented matrix $[\\mathbf{A} | \\mathbf{b}]$ into row echelon form (upper triangular) through row operations, then solves via back substitution. For a system $\\mathbf{Ax} = \\mathbf{b}$ where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, the algorithm proceeds as follows (without pivoting for simplicity):\n",
    "\n",
    "1. Forward Elimination Phase:\n",
    "1.1 For each pivot row $k = 1$ to $n-1$:\n",
    "For each row $i = k+1$ to $n$:\n",
    "Compute the multiplier: $m_{ik} = \\frac{a_{ik}}{a_{kk}}$\n",
    "Update row $i$: $a_{ij} \\leftarrow a_{ij} - m_{ik} a_{kj}$ for $j = k$ to $n$\n",
    "Update right-hand side: $b_i \\leftarrow b_i - m_{ik} b_k$\n",
    "\n",
    "This results in an upper triangular matrix $\\mathbf{U}$ and updated $\\mathbf{b}'$.Symbolically, the elimination step for element $a_{ij}$ (with $i > k, j \\geq k$) is:$$a_{ij}^{(k)} = a_{ij}^{(k-1)} - \\frac{a_{ik}^{(k-1)}}{a_{kk}^{(k-1)}} a_{kj}^{(k-1)}$$where superscript $(k)$ denotes after the $k$-th elimination step.\n",
    "1.2 Back Substitution Phase:\n",
    "Starting from the last equation:$$x_n = \\frac{b_n'}{u_{nn}}$$Then for $i = n-1$ down to $1$:$$x_i = \\frac{b_i' - \\sum_{j=i+1}^n u_{ij} x_j}{u_{ii}}$$\n",
    "\n",
    "This method has a time complexity of $O(n^3)$ due to the nested loops in elimination.\n",
    "\n",
    "2. LU Decomposition Algorithm\n",
    "LU decomposition factors $\\mathbf{A}$ into a lower triangular matrix $\\mathbf{L}$ (with 1s on the diagonal) and an upper triangular matrix $\\mathbf{U}$, such that $\\mathbf{A} = \\mathbf{LU}$. This is equivalent to Gaussian elimination but stores the multipliers in $\\mathbf{L}$. For solving $\\mathbf{Ax} = \\mathbf{b}$, first solve $\\mathbf{Ly} = \\mathbf{b}$ (forward substitution), then $\\mathbf{Ux} = \\mathbf{y}$ (back substitution).\n",
    "\n",
    "2.1 Factorization Phase (Doolittle's Algorithm, without pivoting):\n",
    "Initialize $\\mathbf{L} = \\mathbf{I}$ (identity) and $\\mathbf{U} = \\mathbf{A}$.For each column $k = 1$ to $n$:\n",
    "For the upper part ($j = k$ to $n$): $u_{kj} = a_{kj}$ (already set)\n",
    "For the multipliers ($i = k+1$ to $n$):\n",
    "$$l_{ik} = \\frac{u_{ik}}{u_{kk}}, \\quad u_{ij} \\leftarrow u_{ij} - l_{ik} u_{kj} \\quad \\forall j \\geq k$$\n",
    "More formally, for $k = 1$ to $n$:$$u_{kk} = a_{kk} - \\sum_{m=1}^{k-1} l_{km} u_{mk}$$For $j = k+1$ to $n$:$$u_{kj} = a_{kj} - \\sum_{m=1}^{k-1} l_{km} u_{mj}$$For $i = k+1$ to $n$:$$l_{ik} = \\frac{1}{u_{kk}} \\left( a_{ik} - \\sum_{m=1}^{k-1} l_{im} u_{mk} \\right)$$\n",
    "2.2 Forward Substitution ($\\mathbf{Ly} = \\mathbf{b}$):$$y_1 = \\frac{b_1}{l_{11}} = b_1 \\quad (\\text{since } l_{11} = 1)$$For $i = 2$ to $n$:$$y_i = b_i - \\sum_{j=1}^{i-1} l_{ij} y_j$$\n",
    "2.3 Back Substitution ($\\mathbf{Ux} = \\mathbf{y}$):\n",
    "Identical to Gaussian elimination's back substitution phase.\n",
    "\n",
    "LU decomposition also has $O(n^3)$ complexity for factorization but allows efficient reuse for multiple $\\mathbf{b}$ vectors (each solve is $O(n^2)$).\n",
    "Note on Pivoting: In practice, partial pivoting is added to both methods for numerical stability, swapping rows to select the largest pivot and avoid division by small numbers. This modifies the algorithms to produce $\\mathbf{PA} = \\mathbf{LU}$ where $\\mathbf{P}$ is a permutation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa82223b26c815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6f9b806394ea31",
   "metadata": {},
   "source": [
    "### Exercise 7: Comparing Direct Linear Solvers (LU vs. SVD)\n",
    "In this exertcise we evaluate stability and performance of direct solvers for ill-conditioned systems.\n",
    "Problem: Generate a Hilbert matrix $\\mathbf{H}_n$ (ill-conditioned) of size $n=10$:\n",
    "$$H_{ij} = \\frac{1}{i + j - 1}$$\n",
    "\n",
    "Solve $\\mathbf{Hx} = \\mathbf{b}$ (let $\\mathbf{b} = \\mathbf{H} \\cdot \\mathbf{1}$, true $\\mathbf{x} = \\mathbf{1}$) using LU decomposition (scipy.linalg.lu_solve) and SVD (np.linalg.pinv).\n",
    "Perturb $\\mathbf{b}$ slightly and compare recovered $\\mathbf{x}$ errors.\n",
    "Measure computation time for increasing $n$.\n",
    "\n",
    "Hints: Use scipy.linalg.lu_factor for LU. SVD is more robust for rank-deficient cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789091179051a6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8831dcf35626724",
   "metadata": {},
   "source": [
    "### Exercise 8: Implementing and Testing Gauss-Seidel Iteration\n",
    "\n",
    "Objective: Implement an iterative solver and analyze convergence conditions.\n",
    "Problem: Solve the system from the chapter example:\n",
    "$$\\mathbf{A} = \\begin{pmatrix} 4 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 4 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 2 \\end{pmatrix}.$$\n",
    "\n",
    "Implement Gauss-Seidel with a tolerance of 1e-6 and max 100 iterations.\n",
    "Test on a non-diagonally dominant matrix (e.g., swap rows) and observe divergence.\n",
    "Add SOR with $\\omega = 1.2$ and compare iteration counts.\n",
    "\n",
    "Hints: Check diagonal dominance: $|a_{ii}| \\geq \\sum_{j \\neq i} |a_{ij}|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616cea0c5212f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
