{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62291b6",
   "metadata": {},
   "source": [
    "# Chapter 13: Control IV – Reinforcement Learning Foundations\n",
    "\n",
    "## 13.1  Motivation: From Classical & Optimal Control to Learning-Based Control\n",
    "\n",
    "\n",
    "In previous chapters, we explored classical and optimal control techniques, such as PID controllers, Linear Quadratic Regulators (LQR), Linear Quadratic Gaussian (LQG) estimators, and Model Predictive Control (MPC). These methods rely on well-defined mathematical models of the system dynamics, often assuming linearity, full state observability, or known disturbances. For instance, PID controllers use proportional, integral, and derivative terms to minimize tracking errors in feedback loops, while LQR optimizes quadratic cost functions for linear systems, and MPC solves constrained optimization problems over a receding horizon. LQG extends LQR to handle noisy measurements via Kalman filtering.\n",
    "\n",
    "Reinforcement Learning (RL), on the other hand, represents a paradigm shift toward learning-based control. Unlike model-based methods like MPC, which require an explicit system model (e.g., $  \\dot{\\mathbf{x}} = \\mathbf{Ax} + \\mathbf{Bu}  $), RL agents learn optimal behaviors through trial-and-error interactions with the environment, guided by rewards. This makes RL particularly advantageous in handling uncertainty, non-linearity, and partial models: common challenges in space engineering.\n",
    "\n",
    "For example, in orbit control for satellites, classical PID might struggle with unmodeled gravitational perturbations or sensor noise, while LQR/LQG assumes linear dynamics that may not hold for large maneuvers. MPC can address non-linearities but demands accurate models and significant computational resources for real-time optimization. RL shines here by learning policies that adapt to uncertainties, such as in autonomous station-keeping where the agent can discover fuel-efficient strategies without a perfect dynamics model. Similarly, in space robotics (e.g., manipulator arms on the ISS), RL can handle high-dimensional, non-linear kinematics with partial observability, outperforming traditional methods when models are incomplete or environments are stochastic.\n",
    "\n",
    "Overall, RL bridges the gap between control theory and machine learning, enabling robust performance in complex, real-world scenarios where traditional approaches may falter due to modeling limitations.617ms\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 13.1.1  When RL Shines in Engineering\n",
    "\n",
    "Model-based control techniques (PID, LQR, MPC, etc.) perform extremely well when we possess an accurate, low-to-moderate-dimensional dynamical model and the disturbances are either known or can be adequately captured by simple stochastic descriptions. In many real engineering systems, these assumptions break down in one or more of the following ways:\n",
    "\n",
    "- **Incomplete or highly uncertain dynamics:** Atmospheric drag on low-Earth-orbit satellites varies strongly with solar activity and vehicle attitude; flexible appendages introduce unmodeled structural modes; plume impingement in formation flying couples vehicles in ways that are difficult to predict analytically.\n",
    "- **High-dimensional or partially observable state spaces:** A planetary rover has dozens of joint angles, wheel slip ratios, and terrain parameters that are never perfectly known. A robotic manipulator on a space station must reason about contact forces, micro-gravity floating base dynamics, and visual occlusions—state dimensions easily exceed hundreds or thousands.\n",
    "- **Complex, long-horizon objectives with sparse rewards:** Autonomous docking, debris removal, or in-orbit servicing require sequences of hundreds or thousands of actions where meaningful feedback (success/failure) arrives only at the end of the episode. Classical methods struggle to propagate credit through such long horizons without an excellent predictive model.\n",
    "- **Multi-agent and game-theoretic interactions:** Satellite constellations with collision-avoidance negotiations, or cooperative assembly tasks, introduce non-stationary dynamics because each agent’s policy affects the others.\n",
    "\n",
    "In these regimes, reinforcement learning excels because the agent can **discover effective strategies directly from data**, even when the true equations of motion are unknown, high-dimensional, or stochastic. Instead of hand-crafting a model and solving an optimization problem online, the RL agent iteratively refines a policy that maps (possibly partial) observations to actions, gradually learning to exploit regularities that were never explicitly modeled.\n",
    "\n",
    "Some space relevant examples where RL has already shown promise:\n",
    "\n",
    "- Adaptive station-keeping under uncertain $  J_2  $, drag, and solar-radiation pressure\n",
    "- Autonomous spacecraft docking and rendezvous with minimal fuel (NASA’s work on RL-based guidance)\n",
    "- Planetary rover path planning on unknown, slippery regolith\n",
    "- Dexterous manipulation with soft robotic grippers for on-orbit servicing\n",
    "\n",
    "### 13.1.2  Limitations and Practical Considerations\n",
    "\n",
    "Thank you for the correction: I've revised the discussion on sample inefficiency to reflect a more nuanced view. While early deep reinforcement learning (RL) methods (e.g., DQN or PPO in high-dimensional tasks like Atari games) indeed often required millions or billions of environment interactions to converge from scratch, recent advances in sample-efficient RL, especially for robotics, have dramatically reduced this requirement. In many cases, particularly when incorporating expert demonstrations, priors, or curricula, RL can learn effective policies from just a few episodes or even a single successful demonstration. For instance, methods like self-imitation RL (SIRL), demonstration-guided exploration, or LLM-augmented RL (e.g., RLingua) enable convergence in under 50 episodes or generations by leveraging structured knowledge or bootstrapping from sparse successes.\n",
    "\n",
    "Despite its strengths, reinforcement learning is _not_ a drop-in replacement for classical or optimal control. Engineers adopting RL must be aware of the following fundamental challenges:\n",
    "\n",
    "| Challenge                          | Description                                                                 | Space-Specific Implications                                                                 |\n",
    "|------------------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|\n",
    "| Sample inefficiency (in pure from-scratch settings) | While modern variants with demonstrations or priors can learn from few interactions, baseline model-free RL often requires extensive data to explore high-dimensional spaces effectively. | Real spacecraft cannot execute millions of maneuvers; simulation-reality gap (\"Sim2Real\") remains critical even with efficient methods. |\n",
    "| Exploration risk                   | Random exploration can drive the system into unsafe or destructive states.  | In space, a single bad thrust can de-tumble a satellite or cause collision; physical testing is essentially impossible. |\n",
    "| Reward design fragility            | Poorly shaped rewards lead to unintended behavior (reward hacking).         | Sparse rewards (e.g., \"+1 if docked, 0 otherwise\") yield almost no learning signal for long-horizon missions. |\n",
    "| Non-stationarity & brittleness     | Policies trained in one environment often fail when parameters drift (e.g., mass change after fuel depletion). | Orbital environments evolve (drag coefficient changes, sensor degradation); policies must remain robust or adapt online. |\n",
    "| Lack of formal guarantees          | Unlike LQR or robust MPC, most RL methods offer no hard stability or constraint-satisfaction certificates. | Certification for flight software is a major barrier in aerospace. |\n",
    "\n",
    "These difficulties explain why pure end-to-end deep RL is still rare on actual flight hardware today. Instead, successful engineering deployments almost always combine RL with prior knowledge: hierarchical architectures, safety shields, model-based initialization, sim2real transfer techniques (domain randomization, system identification), and reward shaping informed by control theory.\n",
    "\n",
    "The remainder of this chapter (and the next) will equip you with the foundational tools—Markov decision processes, dynamic programming, temporal-difference learning, function approximation, and policy gradients—so that you can understand when and how to responsibly apply RL in computational engineering practice, and where classical methods remain the safer, more efficient choice.\n",
    "\n"
   ],
   "id": "81671656d482a02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.2  Overview of Reinforcement Learning Methods\n",
    "\n",
    "(from Brunton & Kutz, Ch. 11)\n",
    "\n",
    "[Incorporate Figure 11.3 from Brunton & Kutz: rough categorization of RL into model-based vs. model-free, gradient-based vs. gradient-free, on-policy vs. off-policy; explain dichotomies with brief definitions; include the figure description and key equations like MDP, policy/value iteration, Q-learning, etc.; no code]\n",
    "\n",
    "For an overview of reinforcement learning (RL) methods from an engineering perspective, we follow the treating in Chapter 11 of Data-Driven Science and Engineering by Brunton and Kutz provides a comprehensive overview of reinforcement learning (RL) methods, emphasizing their connections to dynamical systems and control theory. A key visual aid is Figure 11.3, which presents a rough categorization of RL algorithms along several dichotomies. This figure helps navigate the diverse landscape of RL techniques by organizing them into overlapping categories based on their underlying principles.\n",
    "\n",
    "Description of Figure 11.3\n",
    "Figure 11.3 is a schematic diagram (often depicted as intersecting axes or Venn-like regions) that classifies RL methods across three primary dimensions:\n",
    "\n",
    "\n",
    "- **Model-based vs. Model-free:** One axis distinguishes methods that explicitly build or use a model of the environment's dynamics from those that learn directly from experience without a model.\n",
    "- **Gradient-based vs. Gradient-free:** Another dimension separates algorithms that optimize using gradients (e.g., via backpropagation) from those that do not require derivatives, such as tabular or evolutionary approaches.\n",
    "- **On-policy vs. Off-policy:** A third aspect differentiates learning from data generated by the current policy (on-policy) versus data from any policy (off-policy, allowing reuse of old experiences).\n",
    "\n",
    "The figure labels representative algorithms in each category or intersection, such as:\n",
    "\n",
    "- Model-based methods (e.g., Dyna, Model Predictive Control with learned models).\n",
    "- Model-free methods (e.g., Q-learning, SARSA).\n",
    "- Gradient-based (e.g., Policy Gradients, Actor-Critic).\n",
    "- Gradient-free (e.g., Tabular Q-learning, Genetic Algorithms).\n",
    "- On-policy (e.g., SARSA, REINFORCE).\n",
    "- Off-policy (e.g., Q-learning, DQN).\n",
    "\n",
    "Key equations are highlighted around the diagram, including the Markov Decision Process (MDP) tuple, Bellman equations for value and policy iteration, and the Q-learning update rule. The figure underscores how these categories are not mutually exclusive—many modern algorithms (e.g., Deep Deterministic Policy Gradient, DDPG) blend elements from multiple categories. (Note: For the exact visual, refer to the book; here, we provide a textual summary for clarity.)\n",
    "\n",
    "To illustrate the categorization conceptually, consider the following table representing the intersections (inspired by Figure 11.3):\n",
    "\n",
    "| Category        | Model-Based Examples                                      | Model-Free Examples                                             |\n",
    "|-----------------|-----------------------------------------------------------|-----------------------------------------------------------------|\n",
    "| Gradient-Based  | Model-based policy optimization (e.g., PILCO)             | Policy gradients (e.g., REINFORCE, PPO); Actor-Critic (A2C/A3C) |\n",
    "| Gradient-Free   | Model-based planning (e.g., MCTS with learned models)     | Tabular methods (e.g., Q-learning, SARSA); Evolutionary strategies |\n",
    "| On-Policy Subset| On-policy model-based (e.g., some MPC variants)           | SARSA, REINFORCE                                                |\n",
    "| Off-Policy Subset| Off-policy model-based (e.g., Dyna-Q)                    | Q-learning, DQN                                                 |\n",
    "\n",
    "\n",
    "Explanations of Key Dichotomies\n",
    "Below, we briefly define each dichotomy, tying back to the figure's structure:\n",
    "\n",
    "- **Model-Based vs. Model-Free:**\n",
    "Model-based RL explicitly learns or uses a transition model $  P(s' | s, a)  $ and reward function $  R(s, a, s')  $ to plan actions (e.g., via simulation or optimization). This connects to optimal control concepts like the Hamilton-Jacobi-Bellman (HJB) equation. Model-free methods, in contrast, learn value functions or policies directly from trial-and-error data without building a model, making them suitable for complex, high-dimensional environments where modeling is intractable.\n",
    "- **Gradient-Based vs. Gradient-Free:**\n",
    "Gradient-based methods compute derivatives of a loss or objective (e.g., policy gradient theorem: $  \\nabla_\\theta J(\\theta) = \\mathbb{E} [\\nabla_\\theta \\log \\pi_\\theta(a|s) \\hat{A}(s,a)]  $) to update parameters, often using neural networks. Gradient-free approaches avoid derivatives, relying on finite differences, evolutionary algorithms, or tabular lookups—useful when the objective is non-differentiable or black-box.\n",
    "- **On-Policy vs. Off-Policy:**\n",
    "On-policy learning evaluates and improves the same policy that generates data (e.g., SARSA update: $  Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma Q(s',a') - Q(s,a)]  $, where $  a'  $ comes from the current policy). Off-policy methods learn from data generated by a different (behavior) policy, enabling better data reuse and exploration (e.g., Q-learning: $  Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]  $).\n",
    "\n",
    "\n",
    "The figure integrates foundational equations for RL, rooted in the Markov Decision Process (MDP) framework:\n",
    "\n",
    "- MDP Definition: An MDP is defined by the tuple $  (S, A, P, R, \\gamma)  $, where $  S  $ is the state space, $  A  $ the action space, $  P(s'|s,a)  $ the transition probabilities, $  R(s,a,s')  $ the reward, and $  \\gamma \\in [0,1)  $ the discount factor.\n",
    "- Bellman Expectation Equation (for value function under policy $  \\pi  $):\n",
    "$  V^\\pi(s) = \\sum_a \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]  $.\n",
    "- Bellman Optimality Equation (for optimal value $  V^*  $):\n",
    "$  V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]  $.\n",
    "- Policy Iteration: Alternates evaluation (solve Bellman expectation) and improvement (greedy update: $  \\pi(s) = \\arg\\max_a Q(s,a)  $).\n",
    "- Value Iteration: Iteratively applies the optimality operator: $  V^{k+1}(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^k(s') \\right]  $.\n",
    "- Q-Learning Update (off-policy, model-free): As above, converging to the optimal action-value $  Q^*(s,a)  $.\n",
    "\n",
    "These equations form the mathematical backbone, with the figure showing how algorithms like policy/value iteration fit into dynamic programming (model-based, known MDP), while Q-learning extends to unknown environments (model-free). This sets the stage for deeper dives in subsequent sections.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "23b64f43a01cabf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 13.2.1  Model-Based vs. Model-Free RL\n",
    "\n",
    "In reinforcement learning (RL), a fundamental distinction is between model-based and model-free approaches, which differ in how they handle the environment's dynamics and learn optimal behaviors. model-based methods are positioned as leveraging explicit models for planning, while model-free methods rely on direct interaction data.\n",
    "\n",
    "**Model-based RL** involves learning or using an explicit model of the environment's dynamics, typically represented as a transition function $  P(s' | s, a)  $ (next state given current state and action) and reward function $  R(s, a, s')  $. In engineering contexts, this often mirrors classical control formulations, such as the state-space dynamics $  \\dot{\\mathbf{x}} = f(\\mathbf{x}, \\mathbf{u})  $, where $  \\mathbf{x}  $ is the state and $  \\mathbf{u}  $ the control input. The agent uses this model to simulate trajectories, plan ahead, and optimize policies—similar to model predictive control (MPC). Advantages include higher sample efficiency, as the model allows \"mental rehearsals\" without real-world interactions. This links directly to optimal control theory, particularly the Hamilton-Jacobi-Bellman (HJB) equation, which solves for the optimal value function $  V^*(\\mathbf{x}) = \\min_{\\mathbf{u}} \\left[ c(\\mathbf{x}, \\mathbf{u}) + \\gamma \\frac{\\partial V^*}{\\partial \\mathbf{x}} f(\\mathbf{x}, \\mathbf{u}) \\right]  $ (continuous-time variant), where $  c  $ is the cost. Examples include Dyna (which learns a model from experience and uses it for planning) and Probabilistic Inference for Learning Control (PILCO), which uses Gaussian processes for uncertainty-aware modeling. In space engineering, model-based RL could simulate orbital mechanics under perturbations like $  J_2  $ for efficient station-keeping.\n",
    "\n",
    "In contrast, **model-free RL** does not build an explicit dynamics model; instead, it learns value functions (e.g., $  Q(s, a)  $) or policies directly from raw interaction data (states, actions, rewards) through trial-and-error. This data-driven approach is robust to modeling errors and scales to high-dimensional, complex environments where deriving $  f(\\mathbf{x}, \\mathbf{u})  $ analytically is infeasible. It connects to deep RL, where neural networks approximate functions in methods like Deep Q-Networks (DQN) or Proximal Policy Optimization (PPO), enabling end-to-end learning from pixels (e.g., in robotics vision). However, model-free methods can be sample-inefficient, requiring extensive data to converge. The core mechanism is temporal-difference learning, updating estimates based on bootstrapped values without simulating full trajectories. For instance, Q-learning iteratively refines $  Q(s, a)  $ without needing $  P  $ or $  R  $.\n",
    "\n",
    "The trade-off is clear: model-based methods excel in structured, low-data regimes with interpretable dynamics (tying to HJB/optimal control for guarantees), but suffer if the model is inaccurate or expensive to learn. Model-free methods shine in unstructured, high-dimensional settings (leveraging deep RL for generalization), but demand more interactions—motivating hybrids like Model-Based Acceleration (MBA) that combine both for space applications, such as adaptive control under uncertain atmospheric drag.\n",
    "\n",
    "\n"
   ],
   "id": "3f34a108070bbe85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13.2.2  Gradient-Based vs. Gradient-Free Methods\n",
    "\n",
    "Another key dichotomy in reinforcement learning (RL), as highlighted in Figure 11.3 of Brunton and Kutz (Chapter 11), is between gradient-based and gradient-free methods. This classification focuses on how algorithms optimize policies or value functions—either by computing explicit gradients for updates or by using derivative-free techniques. Gradient-based approaches are often associated with continuous, differentiable parameterizations (e.g., neural networks), while gradient-free methods are more versatile for discrete or non-differentiable settings.\n",
    "\n",
    "**Gradient-based** RL methods optimize a parameterized objective directly using gradients, typically via automatic differentiation and backpropagation. A prime example is policy gradient methods, such as REINFORCE, where the policy $  \\pi_\\theta(a|s)  $ (parameterized by $  \\theta  $, e.g., neural network weights) is updated to maximize the expected return $  J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} [R(\\tau)]  $, with the gradient given by the policy gradient theorem: $  \\nabla_\\theta J(\\theta) = \\mathbb{E} [\\nabla_\\theta \\log \\pi_\\theta(a|s) G_t]  $, where $  G_t  $ is the return from time $  t  $. This enables smooth updates in high-dimensional continuous action spaces, linking to stochastic gradient descent (SGD) in machine learning. Pros include high efficiency in parameter-rich models (e.g., deep networks), as gradients provide directed improvements, leading to faster convergence in smooth landscapes. They are particularly applicable in engineering tasks like continuous control in space systems, such as optimizing thrust vectors for satellite attitude adjustment, where policies can be fine-tuned for precision. However, cons involve sensitivity to non-differentiable elements (e.g., discrete actions or hard constraints) and potential instability from noisy gradients in sparse-reward environments.\n",
    "\n",
    "In contrast, **gradient-free** RL methods avoid computing derivatives altogether, relying on sampling, perturbations, or tabular updates to explore and evaluate options. Classic examples include Q-learning (a gradient-free value-based method), which updates the action-value function $  Q(s,a)  $ via temporal differences without gradients: $  Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]  $, or evolutionary strategies that treat policies as black-box functions and evolve populations via mutations. These are often tabular or use finite differences for approximation. Pros emphasize broad applicability: they handle non-differentiable, discrete, or black-box objectives seamlessly, making them robust in uncertain or hybrid systems (e.g., spacecraft with switched modes like thruster on/off). They also avoid local optima traps common in gradient descent by leveraging global exploration. Cons center on lower efficiency, especially in high-dimensional spaces, as they require many evaluations (e.g., rollouts) without directional guidance, leading to higher sample complexity—critical in space engineering where simulations are computationally expensive.\n",
    "\n",
    "Overall, gradient-based methods (e.g., policy gradients like PPO or TRPO) excel in efficiency for scalable, continuous problems with deep RL integration, but demand differentiability. Gradient-free approaches (e.g., Q-learning or cross-entropy methods) prioritize flexibility and robustness, suiting discrete or model-agnostic scenarios, though at the cost of data hunger. Hybrids, such as gradient-free wrappers around gradient-based cores, offer compromises for real-world applications like autonomous rover navigation under variable terrain.\n"
   ],
   "id": "72bf0b8ff9215c2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13.2.3  On-Policy vs. Off-Policy Learning\n",
    "\n",
    "A third important dichotomy in reinforcement learning (RL), as illustrated in Figure 11.3 of Brunton and Kutz (Chapter 11), is between on-policy and off-policy methods. This classification centers on the relationship between the policy used to generate data (for exploration and interaction) and the policy being learned or improved. On-policy approaches tie learning directly to the current policy's behavior, while off-policy methods decouple data generation from policy optimization, enabling more flexible use of experiences. This distinction has significant implications for exploration strategies and data efficiency, particularly in engineering applications where real-world interactions are costly or risky.\n",
    "On-policy RL methods learn and evaluate the value function or policy using data generated exclusively by the current policy being optimized. In other words, the agent acts according to its present policy $  \\pi  $, collects trajectories (states, actions, rewards), and updates $  \\pi  $ based on those same trajectories. This creates a self-consistent loop where improvements are grounded in the policy's own behavior. Key examples include:\n",
    "\n",
    "- SARSA (State-Action-Reward-State-Action): An on-policy temporal-difference (TD) method for action-value learning. The update rule is $  Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma Q(s',a') - Q(s,a)]  $, where $  a'  $ is sampled from the current policy $  \\pi  $ (e.g., ε-greedy). This ensures the update reflects the policy's exploratory actions.\n",
    "- TD(0): A basic on-policy algorithm for state-value estimation, updating $  V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)]  $ based on samples from $  \\pi  $.\n",
    "\n",
    "On-policy methods promote consistent learning but require the policy itself to handle exploration (e.g., via softmax or ε-greedy), which can lead to suboptimal data if exploration is too aggressive or conservative. Data usage is limited to fresh trajectories from the current $  \\pi  $, making these methods less sample-efficient but easier to implement with guarantees of convergence under certain conditions.\n",
    "\n",
    "In contrast, off-policy RL methods learn a target policy $  \\pi  $ from data generated by a different behavior policy $  \\mu  $ (where $  \\mu \\neq \\pi  $). This separation allows the agent to explore broadly with $  \\mu  $ (e.g., a random or exploratory policy) while optimizing $  \\pi  $ toward optimality (e.g., greedy). Corrections like importance sampling ratios $  \\frac{\\pi(a|s)}{\\mu(a|s)}  $ adjust for the policy mismatch. Prominent examples are:\n",
    "\n",
    "- Q-learning: A foundational off-policy TD method, with the update $  Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]  $. Here, the max assumes a greedy target policy, even if the behavior policy is exploratory.\n",
    "- Deep Q-Networks (DQN): An off-policy deep RL extension of Q-learning, using neural networks for $  Q  $-approximation, experience replay buffers to store and reuse diverse data, and target networks for stability.\n",
    "\n",
    "Off-policy approaches enhance exploration by allowing $  \\mu  $ to be designed independently for safety or coverage (e.g., more random in simulations), while $  \\pi  $ focuses on exploitation. For data usage, they excel in efficiency by replaying historical experiences from any policy, reducing the need for constant new interactions—critical in high-cost domains like space engineering.\n",
    "\n",
    "Implications for Exploration and Data Usage:\n",
    "\n",
    "- Exploration: On-policy methods integrate exploration into the policy, which can bias learning toward safer but suboptimal paths (e.g., SARSA learns a \"cautious\" policy avoiding cliffs in gridworlds). Off-policy methods (e.g., Q-learning) enable aggressive exploration via $  \\mu  $ without corrupting $  \\pi  $, ideal for space scenarios like satellite maneuvering where real trials risk collisions but simulations can explore freely.\n",
    "- Data Usage: On-policy requires discarding old data when $  \\pi  $ changes, leading to waste in dynamic environments. Off-policy leverages replay buffers for better sample efficiency, though it introduces variance from importance sampling. In practice, off-policy is preferred for data-scarce engineering tasks (e.g., rover path planning with limited battery life), but on-policy offers simplicity and stability in well-modeled systems.\n",
    "\n",
    "This flexibility makes off-policy methods like DQN prevalent in deep RL for complex, partially observable space applications, while on-policy variants suit iterative refinement in known dynamics. Hybrids, such as importance-sampled actor-critic, often balance these trade-offs in real-world deployments."
   ],
   "id": "2703df73b3a100d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ## 13.3  Markov Decision Processes (MDPs) – The Mathematical Framework\n",
    "\n",
    "Markov Decision Processes (MDPs) provide the foundational mathematical framework for reinforcement learning (RL), formalizing sequential decision-making under uncertainty. An MDP models an agent's interaction with an environment as a cycle of observing states, taking actions, receiving rewards, and transitioning to new states. This setup captures the essence of control problems where decisions affect future outcomes, making it ideal for engineering applications like autonomous systems in space.\n",
    "\n",
    "At its core, an MDP is defined by a tuple $  (S, A, P, R, \\gamma)  $, where each element represents a key aspect of the decision process. The \"Markov\" property ensures that the future state depends only on the current state and action, not on prior history—simplifying analysis while still allowing rich dynamics.\n",
    "\n",
    "A simple space engineering example is satellite attitude control: The satellite must maintain a desired orientation despite disturbances like magnetic torques. Here, the state could include current angles and angular velocities, actions might be thruster firings, transitions account for orbital dynamics, rewards penalize deviation from the target attitude and fuel use, and discounting prioritizes short-term stability.\n",
    "\n",
    "Below is a conceptual diagram of the MDP interaction loop:\n",
    "\n",
    "\n",
    "<img src=\"../images/13_1_mermaid-diagram.svg\"\n",
    "     alt=\"This loop repeats: The agent selects actions to maximize long-term rewards, learning from experience.\"\n",
    "     width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "### 13.3.1  Components of an MDP\n",
    "\n",
    "[Detailed breakdown: state space S, action space A, transition P(s'|s,a), reward R(s,a,s'), discount γ; mathematical notation; no code]\n",
    "\n",
    "- State Space $  S  $: The set of all possible states the environment can be in. States encode relevant information for decision-making. In continuous spaces (common in engineering), $  S \\subseteq \\mathbb{R}^n  $; in discrete, it's a finite set. For the satellite example, $  s = [\\theta, \\dot{\\theta}]^\\top  $, where $  \\theta  $ is attitude error and $  \\dot{\\theta}  $ is rate.\n",
    "- Action Space $  A  $: The set of all possible actions the agent can take. Actions can be discrete (e.g., fire thruster left/right) or continuous (e.g., torque magnitude $  u \\in \\mathbb{R}^m  $). In attitude control, $  a  $ might be a vector of control torques.\n",
    "- Transition Dynamics $  P(s' | s, a)  $: The probability distribution over next states $  s'  $ given current state $  s  $ and action $  a  $. This models the environment's response, including uncertainty (e.g., stochastic disturbances). In deterministic cases, it's a function $  s' = f(s, a)  $; probabilistically, $  P  $ captures noise like sensor errors in space.\n",
    "- Reward Function $  R(s, a, s')  $: A scalar signal indicating the immediate desirability of the transition. Rewards guide learning: positive for good outcomes (e.g., +1 for stable attitude), negative for bad (e.g., -fuel cost or -deviation penalty). In engineering, rewards often encode objectives like minimizing energy while achieving goals.\n",
    "- Discount Factor $  \\gamma \\in [0, 1)  $: Discounts future rewards to emphasize short-term gains, ensuring convergence in infinite-horizon problems. A $  \\gamma  $ close to 1 values long-term planning (e.g., fuel-efficient orbits); closer to 0 prioritizes immediacy (e.g., emergency maneuvers). The total return is $  G_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1}  $.\n",
    "\n",
    "### 13.3.2  Finite vs. Infinite MDPs\n",
    "\n",
    "MDPs can be finite (discrete, episodic tasks with termination, like docking maneuvers ending on success/failure) or infinite (continuous spaces, ongoing tasks like perpetual station-keeping). Finite MDPs suit tabular methods; infinite require approximations (e.g., function approximators for high-dimensional space states). In space applications, hybrid forms are common: episodic for mission phases, continuing for long-duration operations. This framework enables RL to solve underdetermined problems where dynamics are partially known or stochastic.\n",
    "\n"
   ],
   "id": "f927e0afcfcea45d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.4  Policies, Value Functions, and Bellman Equations\n",
    "\n",
    "In reinforcement learning (RL), the agent's decision-making strategy is captured by a policy, while value functions quantify the long-term desirability of states or state-action pairs under that policy. These concepts build on the MDP framework, enabling the agent to evaluate and improve behaviors to maximize cumulative rewards. Policies map states to actions, and value functions provide a scalar measure of expected future rewards, forming the basis for learning algorithms.\n",
    "\n",
    "A policy $  \\pi(a|s)  $ specifies the probability of taking action $  a  $ in state $  s  $. The state-value function $  V^\\pi(s)  $ estimates the expected return starting from state $  s  $ and following policy $  \\pi  $:\n",
    "$   V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} \\bigg| s_t = s \\right],   $\n",
    "where the expectation is over trajectories generated by $  \\pi  $, and $  \\gamma  $ is the discount factor.\n",
    "\n",
    "The action-value function $  Q^\\pi(s,a)  $ similarly estimates the expected return starting from $  s  $, taking action $  a  $, and then following $  \\pi  $:\n",
    "$   Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} \\bigg| s_t = s, a_t = a \\right].   $\n",
    "These relate via $  V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)  $.\n",
    "\n",
    "The Bellman expectation equation expresses value functions recursively, leveraging the MDP's Markov property:\n",
    "\n",
    "For $  V^\\pi  $:\n",
    "$   V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} P(s',r | s,a) \\left[ r + \\gamma V^\\pi(s') \\right],   $\n",
    "where the inner sum is over possible next states and rewards (often simplified if rewards are deterministic).\n",
    "\n",
    "For $  Q^\\pi  $:\n",
    "$   Q^\\pi(s,a) = \\sum_{s', r} P(s',r | s,a) \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a') \\right].   $\n",
    "\n",
    "The Bellman optimality equation defines optimal values $  V^*(s) = \\max_\\pi V^\\pi(s)  $ and $  Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)  $:\n",
    "\n",
    "$   V^*(s) = \\max_a \\sum_{s', r} P(s',r | s,a) \\left[ r + \\gamma V^*(s') \\right],   $\n",
    "$   Q^*(s,a) = \\sum_{s', r} P(s',r | s,a) \\left[ r + \\gamma \\max_{a'} Q^*(s',a') \\right].   $\n",
    "\n",
    "These equations are derived by noting that optimal policies choose actions greedily with respect to the optimal values, forming a system of nonlinear equations solvable via iterative methods (as in dynamic programming). In engineering contexts, like spacecraft control, these enable computing optimal thrust policies under fuel constraints.\n",
    "\n",
    "### 13.4.1  Policy Types and Evaluation\n",
    "\n",
    "Policies come in two main types: deterministic and stochastic. A deterministic policy $  \\pi(s)  $ maps each state $  s  $ to a single action $  a  $, i.e., $  \\pi(a|s) = 1  $ for one $  a  $ and 0 otherwise—useful in predictable environments like precise orbital maneuvers where exploration is minimal. A stochastic policy $  \\pi(a|s)  $ assigns probabilities to actions, enabling inherent exploration (e.g., via softmax over Q-values), which is crucial in uncertain settings like rover navigation on variable terrain to avoid local optima.\n",
    "Policy evaluation computes $  V^\\pi  $ or $  Q^\\pi  $ for a fixed policy, often using the Bellman expectation equation iteratively. Starting with an initial guess (e.g., $  V_0(s) = 0  $), updates are:\n",
    "$   V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s', r} P(s',r | s,a) \\left[ r + \\gamma V_k(s') \\right].   $\n",
    "This converges to $  V^\\pi  $ as $  k \\to \\infty  $ for finite MDPs (by contraction mapping). In practice, it's used in policy iteration or as a subroutine in actor-critic methods, evaluating how well a policy performs in tasks like attitude stabilization.\n",
    "\n",
    "### 13.4.2  Optimal Policies and Values\n",
    "\n",
    "An optimal policy $  \\pi^*  $ achieves the highest possible values: $  V^{\\pi^*}(s) = V^*(s)  $ for all $  s  $. Multiple optimal policies may exist, but all share the same optimal values. The Bellman optimality equation provides a way to solve for $  V^*  $ and $  Q^*  $ without enumerating policies, by treating it as a fixed-point problem.\n",
    "Solving involves methods like value iteration: Initialize $  V_0(s) = 0  $, then:\n",
    "\n",
    "$   V_{k+1}(s) = \\max_a \\sum_{s', r} P(s',r | s,a) \\left[ r + \\gamma V_k(s') \\right],   $\n",
    "\n",
    "converging to $  V^*  $. Once $  V^*  $ (or $  Q^*  $) is known, an optimal policy is extracted greedily: $  \\pi^*(s) = \\arg\\max_a Q^*(s,a)  $. For Q-values, the update is analogous.\n",
    "In space engineering, this yields fuel-optimal trajectories (e.g., for rendezvous), but requires a known MDP model. In unknown environments, approximations extend this to model-free RL, balancing computation with performance.\n",
    "\n"
   ],
   "id": "8cf1ac33ef00728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.5  Dynamic Programming – Solving Known MDPs\n",
    "\n",
    "[Policy evaluation, improvement, iteration; value iteration; from Brunton & Kutz; small gridworld example; **Python example – tabular policy iteration on small MDP like frozen lake**]\n",
    "Dynamic Programming (DP) methods provide exact solutions for MDPs where the model (transitions $  P  $ and rewards $  R  $) is fully known. These algorithms break down the Bellman equations into iterative procedures to compute optimal values and policies, serving as a foundation for more advanced RL in unknown environments. In engineering, DP is useful for offline planning in simulated systems, like optimizing spacecraft trajectories with known orbital dynamics.\n",
    "\n",
    "### Policy Evaluation\n",
    "\n",
    "Policy evaluation computes the value function $  V^\\pi  $ for a fixed policy $  \\pi  $, solving the linear system from the Bellman expectation equation:\n",
    "$   V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^\\pi(s') \\right].   $\n",
    "Iteratively, start with $  V_0(s) = 0  $ and update:\n",
    "$   V_{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V_k(s') \\right],   $\n",
    "until convergence (e.g., $  \\|V_{k+1} - V_k\\| < \\epsilon  $). This evaluates policy performance, e.g., assessing fuel efficiency in a given satellite control strategy.\n",
    "\n",
    "### Policy Improvement\n",
    "Given $  V^\\pi  $, improve the policy greedily: For each $  s  $, set $  \\pi'(s) = \\arg\\max_a Q^\\pi(s,a)  $, where\n",
    "$   Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V^\\pi(s') \\right].   $\n",
    "This yields a better or equal policy $  \\pi' \\geq \\pi  $, as per the policy improvement theorem—crucial for refining suboptimal initial policies in control tasks.\n",
    "Policy Iteration\n",
    "Policy iteration alternates evaluation and improvement until the policy stabilizes:\n",
    "\n",
    "1. Initialize arbitrary policy $  \\pi_0  $.\n",
    "2. Evaluate: Compute $  V^{\\pi_k}  $.\n",
    "3. Improve: Set $  \\pi_{k+1}(s) = \\arg\\max_a Q^{\\pi_k}(s,a)  $.\n",
    "4. Repeat until $  \\pi_{k+1} = \\pi_k  $.\n",
    "This converges to $  \\pi^*  $ in finite steps for finite MDPs, balancing computation in applications like optimal path planning for rovers.\n",
    "\n",
    "### Value Iteration\n",
    "\n",
    "Value iteration directly solves the Bellman optimality equation iteratively:\n",
    "$   V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma V_k(s') \\right],   $\n",
    "converging to $  V^*  $. Extract $  \\pi^*  $ greedily from $  V^*  $. It's simpler (no full evaluations) but may require more iterations; efficient for large state spaces in simulated space missions.\n",
    "\n",
    "## Example: Small Gridworld Example\n",
    "Consider a 3x3 gridworld (states 0-8, row-major):\n",
    "- Start at 0, goal at 8 (+1 reward), hole at 4 (-1 reward, terminal).\n",
    " - Actions: up, down, left, right (deterministic, but clip at edges).\n",
    "  -\n",
    " $  \\gamma = 0.9  $. DP finds a policy avoiding the hole while reaching the goal efficiently, analogous to rover navigation avoiding craters. Below is a Python implementation of tabular policy iteration on this MDP. (For value iteration, modify the loop to use the max operator directly.)\n",
    "\n",
    "\n",
    "Consider a 3x3 gridworld (states 0-8, row-major): Start at 0, goal at 8 (+1 reward), hole at 4 (-1 reward, terminal). Actions: up, down, left, right (deterministic, but clip at edges). $  \\gamma = 0.9  $. DP finds a policy avoiding the hole while reaching the goal efficiently, analogous to rover navigation avoiding craters.\n",
    "Below is a Python implementation of tabular policy iteration on this MDP, enhanced with a visualization function using Matplotlib to display the grid, policy arrows, and value labels. (For value iteration, modify the loop to use the max operator directly.) When run in a Jupyter notebook, this will produce a plot showing the optimal policy and values overlaid on the grid.\n",
    "\n"
   ],
   "id": "f29fdfa34e916697"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below is a Python implementation of tabular policy iteration on this MDP, enhanced with a visualization function using Matplotlib to display the grid, policy arrows, and value labels. (For value iteration, modify the loop to use the max operator directly.) When run in a Jupyter notebook, this will produce a plot showing the optimal policy and values overlaid on the grid.",
   "id": "8d2b879435d5ade3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch, Rectangle\n",
    "\n",
    "# Define MDP: 3x3 grid, states 0-8\n",
    "num_states = 9\n",
    "num_actions = 4  # 0: up, 1: down, 2: left, 3: right\n",
    "gamma = 0.9\n",
    "theta = 1e-4  # convergence threshold\n",
    "\n",
    "# Transitions: next_state[s, a], assume deterministic\n",
    "next_state = np.array([\n",
    "    # up, down, left, right for each state\n",
    "    [0, 3, 0, 1],  # s0\n",
    "    [1, 4, 0, 2],  # s1\n",
    "    [2, 5, 1, 2],  # s2\n",
    "    [0, 6, 3, 4],  # s3\n",
    "    [4, 7, 3, 5],  # s4 (hole)\n",
    "    [2, 8, 4, 5],  # s5\n",
    "    [3, 6, 6, 7],  # s6\n",
    "    [4, 7, 6, 8],  # s7\n",
    "    [5, 8, 7, 8]   # s8 (goal)\n",
    "])\n",
    "\n",
    "# Rewards: r[s, a] = -0.01 step cost, +1 goal, -1 hole\n",
    "rewards = -0.01 * np.ones((num_states, num_actions))\n",
    "# Goal transitions to 8 give +1, to 4 give -1\n",
    "for s in range(num_states):\n",
    "    for a in range(num_actions):\n",
    "        if next_state[s, a] == 8:\n",
    "            rewards[s, a] = 1.0\n",
    "        elif next_state[s, a] == 4:\n",
    "            rewards[s, a] = -1.0\n",
    "\n",
    "# Terminals\n",
    "terminals = [4, 8]\n",
    "\n",
    "def policy_evaluation(policy, V):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            if s in terminals:\n",
    "                continue\n",
    "            v = V[s]\n",
    "            a = policy[s]\n",
    "            s_next = next_state[s, a]\n",
    "            r = rewards[s, a]\n",
    "            V[s] = r + gamma * V[s_next]\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(policy, V):\n",
    "    policy_stable = True\n",
    "    for s in range(num_states):\n",
    "        if s in terminals:\n",
    "            continue\n",
    "        old_a = policy[s]\n",
    "        Q = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            s_next = next_state[s, a]\n",
    "            r = rewards[s, a]\n",
    "            Q[a] = r + gamma * V[s_next]\n",
    "        policy[s] = np.argmax(Q)\n",
    "        if old_a != policy[s]:\n",
    "            policy_stable = False\n",
    "    return policy, policy_stable\n",
    "\n",
    "# Visualization function\n",
    "def visualize_grid(policy, V, iteration=0):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(0, 3)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_xticks(np.arange(4))\n",
    "    ax.set_yticks(np.arange(4))\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Arrow directions: up, down, left, right\n",
    "    arrows = [(0, 0.3), (0, -0.3), (-0.3, 0), (0.3, 0)]\n",
    "    labels = ['↑', '↓', '←', '→']\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            s = i * 3 + j\n",
    "            x, y = j + 0.5, 2.5 - i  # Center of cell (row-major, flip y for top-left start)\n",
    "\n",
    "            # Background colors: goal green, hole red, others white\n",
    "            color = 'white'\n",
    "            if s == 8:\n",
    "                color = 'lightgreen'\n",
    "            elif s == 4:\n",
    "                color = 'lightcoral'\n",
    "            ax.add_patch(Rectangle((j, 2-i), 1, 1, color=color, alpha=0.5))\n",
    "\n",
    "            if s in terminals:\n",
    "                ax.text(x, y, f'V={V[s]:.2f}', ha='center', va='center', fontsize=10)\n",
    "                continue\n",
    "\n",
    "            # Policy arrow or label\n",
    "            a = policy[s]\n",
    "            if arrows[a] != (0,0):  # Draw arrow if applicable\n",
    "                dx, dy = arrows[a]\n",
    "                ax.add_patch(FancyArrowPatch((x - dx/2, y - dy/2), (x + dx/2, y + dy/2),\n",
    "                                             arrowstyle='->', mutation_scale=20, color='blue'))\n",
    "            else:\n",
    "                ax.text(x, y + 0.2, labels[a], ha='center', va='center', fontsize=20)\n",
    "\n",
    "            # Value label\n",
    "            ax.text(x, y - 0.2, f'V={V[s]:.2f}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "    ax.set_title('Optimal Policy and Values\\n(Goal: Green, Hole: Red) for iteration = {iteration}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Initialize\n",
    "policy = np.zeros(num_states, dtype=int)  # arbitrary initial policy\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "# Policy Iteration\n",
    "iteration = 0\n",
    "while True:\n",
    "    # Visaulize current policy and values:\n",
    "    visualize_grid(policy, V, iteration=iteration)\n",
    "    # Policy Evaluation and Improvement\n",
    "    V = policy_evaluation(policy, V)\n",
    "    policy, stable = policy_improvement(policy, V)\n",
    "    print(f'iteration {iteration}: policy = {policy}, V = {V}')\n",
    "    iteration += 1\n",
    "    if stable:\n",
    "        break\n",
    "    # Visualize\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal Policy (0=up,1=down,2=left,3=right):\", policy)\n",
    "print(\"Optimal Values:\", V)\n",
    "\n"
   ],
   "id": "6d9b0e3973534802",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 13.5.1  Policy Evaluation and Improvement\n",
    "\n",
    "Policy evaluation and improvement are the core building blocks of dynamic programming in MDPs, enabling systematic computation of values and refinement of policies.\n",
    "\n",
    "**Iterative backups** form the basis of policy evaluation. These are recursive updates that \"back up\" value estimates from future states to the current one, exploiting the Bellman expectation equation. Starting from an arbitrary initial value function (often zero), each iteration sweeps through all states, updating $  V(s)  $ based on the expected reward and discounted future value under the current policy. This process is a contraction mapping in the space of value functions, guaranteeing convergence to the true $  V^\\pi  $ in finite MDPs due to the discount factor $  \\gamma < 1  $. In engineering terms, backups propagate long-term consequences (e.g., cumulative fuel costs in orbit control) backward, providing a holistic assessment.\n",
    "\n",
    "**Greedy improvement** then refines the policy by selecting actions that maximize the one-step lookahead value. Given $  V^\\pi  $, compute the action-value $  Q^\\pi(s,a)  $ for each action, and update the policy to $  \\pi'(s) = \\arg\\max_a Q^\\pi(s,a)  $. The policy improvement theorem ensures $  V^{\\pi'} \\geq V^\\pi  $ (with equality only if $  \\pi  $ is already optimal), driving monotonic progress. This greediness assumes a known model, making it efficient for deterministic systems like robotic path planning, but it can get stuck in local optima if not iterated.\n",
    "\n",
    "Together, these steps enable bootstrapping: evaluation uses self-consistent backups, while improvement exploits them for better decisions, forming the foundation for full algorithms.\n",
    "\n"
   ],
   "id": "612f8b4f861ffcef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13.5.2  Policy Iteration Algorithm\n",
    "\n",
    "The policy iteration algorithm combines evaluation and improvement in a loop to find the optimal policy $  \\pi^*  $.\n",
    "\n",
    "Full Algorithm Steps:\n",
    "\n",
    "1. Initialization: Start with an arbitrary policy $  \\pi_0  $ (e.g., random actions) and optional initial values $  V_0(s) = 0  $.\n",
    "2. Policy Evaluation: Compute $  V^{\\pi_k}  $ by iteratively solving the Bellman expectation until convergence (or for a fixed number of sweeps in approximate versions).\n",
    "3. Policy Improvement: For each state, set $  \\pi_{k+1}(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi_k}(s')]  $.\n",
    "4. Convergence Check: If $  \\pi_{k+1} = \\pi_k  $ (no changes), stop; else, set $  k \\leftarrow k+1  $ and repeat from step 2.\n",
    "\n",
    "**Convergence:** In finite MDPs, policy iteration converges to $  \\pi^*  $ in a finite number of iterations (often few, as each improvement is significant). The number is at most $  |A|^{|S|}  $, but practically much less due to monotonicity. This makes it suitable for small-to-medium MDPs in engineering, like discrete approximations of control problems (e.g., quantized satellite states).\n",
    "\n",
    "In the gridworld example from Section 13.5, the code already implements policy iteration. Here's a snippet highlighting the loop:\n",
    "\n",
    "```python\n",
    "# Policy Iteration Loop\n",
    "while True:\n",
    "    V = policy_evaluation(policy, V)  # Step 2: Evaluate\n",
    "    policy, stable = policy_improvement(policy, V)  # Step 3: Improve\n",
    "    if stable:  # Step 4: Check\n",
    "        break\n",
    "```\n",
    "This iterates until stability, yielding the optimal policy and values."
   ],
   "id": "6d16ab56181374de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13.5.3  Value Iteration as a Special Case\n",
    "\n",
    "Value iteration is a streamlined variant of policy iteration, merging evaluation and improvement into a single update rule for faster implementation in some cases.\n",
    "\n",
    "**Differences:** Unlike policy iteration's full evaluation (multiple sweeps until convergence per policy), value iteration performs only one backup per iteration, directly applying the Bellman optimality operator:\n",
    "\n",
    "$   V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V_k(s')].   $\n",
    "No explicit policy is maintained during iteration; instead, it's extracted at the end via $  \\pi^*(s) = \\arg\\max_a Q^*(s,a)  $ from the converged $  V^*  $. This avoids intermediate policies but may require more total iterations since updates are partial. Policy iteration is \"policy-centric\" with potentially fewer outer loops; value iteration is \"value-centric\" and simpler to code.\n",
    "\n",
    "**When to Use:** Choose value iteration for large state spaces where full evaluations are costly (e.g., discretized PDEs in continuum control), or when early stopping with approximate values suffices. Policy iteration shines when evaluations are cheap and quick convergence in policy space is needed (e.g., small MDPs like attitude control modes). Both are exact in the limit, but value iteration's asynchronous variants (prioritized sweeping) adapt well to sparse transitions in space applications.\n",
    "To extend the Python example from Section 13.5 to value iteration, replace the main loop with:"
   ],
   "id": "2198c67d3e7ca5da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Value Iteration\n",
    "V = np.zeros(num_states)\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in range(num_states):\n",
    "        if s in terminals:\n",
    "            continue\n",
    "        v = V[s]\n",
    "        Q = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            s_next = next_state[s, a]\n",
    "            r = rewards[s, a]\n",
    "            Q[a] = r + gamma * V[s_next]\n",
    "        V[s] = np.max(Q)\n",
    "        delta = max(delta, abs(v - V[s]))\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "\n",
    "# Extract policy\n",
    "policy = np.zeros(num_states, dtype=int)\n",
    "for s in range(num_states):\n",
    "    if s in terminals:\n",
    "        continue\n",
    "    Q = np.zeros(num_actions)\n",
    "    for a in range(num_actions):\n",
    "        s_next = next_state[s, a]\n",
    "        r = rewards[s, a]\n",
    "        Q[a] = r + gamma * V[s_next]\n",
    "    policy[s] = np.argmax(Q)\n",
    "\n",
    "\n",
    "\n",
    "# Reuse visualization\n",
    "visualize_grid(policy, V)"
   ],
   "id": "5248cb23280ef9e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This converges to the same optima, demonstrating the equivalence.",
   "id": "472403c8888569b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.6  Model-Free Methods: Monte-Carlo and Temporal-Difference Learning\n",
    "\n",
    "Model-free reinforcement learning methods learn optimal policies and values directly from interaction data without building an explicit model of the environment's dynamics. Two key approaches are Monte-Carlo (MC) methods, which average returns over complete episodes, and Temporal-Difference (TD) learning, which updates estimates incrementally using bootstrapping. MC requires waiting for episode termination to compute full returns, making it unbiased but high-variance, while TD learns from incomplete sequences, reducing variance at the cost of some bias. These are advantageous in unknown or complex environments, such as space systems with unmodeled perturbations (e.g., variable solar radiation on satellites), where deriving transitions analytically is impractical. Model-free methods enable adaptive control, like real-time adjustment in rover navigation or attitude stabilization under uncertainty.\n",
    "\n",
    "### 13.6.1  Monte-Carlo Policy Evaluation and Control\n",
    "\n",
    "Monte-Carlo (MC) methods estimate values by averaging observed returns over many episodes, treating each as a sample from the policy's distribution. For policy evaluation, the state-value $  V(s)  $ is updated as the mean return following first visits (or every visit) to $  s  $. First-visit MC averages only the return from the initial occurrence of $  s  $ per episode to ensure unbiased estimates; every-visit includes all occurrences, potentially faster converging but biased in overlapping trajectories.\n",
    "For control (optimizing the policy), MC extends to action-values $  Q(s,a)  $, but requires exploration to visit all state-action pairs. Techniques include exploring starts (randomly initializing episodes across states/actions, feasible in simulations) or ε-greedy policies (choosing the best action with probability 1-ε, random otherwise, with ε decaying over time).\n",
    "In space engineering, MC control suits episodic tasks like docking maneuvers, where full simulation rollouts estimate fuel-efficient strategies.\n",
    "Below is a Python example of every-visit MC control with ε-greedy exploration on Gymnasium's Blackjack environment (states: player's sum, dealer's card, usable ace; actions: hit/stick; goal: beat dealer without busting).\n",
    "\n",
    "\n"
   ],
   "id": "f01061cf8ebea7e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7a6126a753e6348",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "# Parameters\n",
    "num_episodes = 100000\n",
    "epsilon_start = 0.1\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = (epsilon_start - epsilon_end) / num_episodes\n",
    "gamma = 1.0  # no discount (episodic)\n",
    "eval_every = 10000  # Evaluate every X episodes\n",
    "eval_episodes = 1000  # Number of eval episodes\n",
    "\n",
    "# Q-table: full space (sum 0-31, dealer 0-10, ace 0/1, actions 0/1)\n",
    "Q = np.zeros((32, 11, 2, 2))\n",
    "counts = np.zeros((32, 11, 2, 2))  # for averaging\n",
    "\n",
    "def state_to_index(state):\n",
    "    player_sum, dealer_card, usable_ace = state\n",
    "    return (player_sum, dealer_card, int(usable_ace))\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    idx = state_to_index(state)\n",
    "    return np.argmax(Q[idx])\n",
    "\n",
    "# Track training and eval returns\n",
    "train_avg_returns = []  # Averages every 100 episodes\n",
    "eval_returns = []  # Greedy eval averages\n",
    "\n",
    "# MC Control (first-visit for unbiased)\n",
    "current_epsilon = epsilon_start\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    trajectory = []  # (state, action, reward)\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, current_epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        trajectory.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    # First-visit MC: Track visited s-a\n",
    "    visited = set()\n",
    "    G = 0\n",
    "    for t in reversed(range(len(trajectory))):\n",
    "        state, action, reward = trajectory[t]\n",
    "        G = reward + gamma * G\n",
    "        sa_key = (state, action)\n",
    "        if sa_key not in visited:\n",
    "            visited.add(sa_key)\n",
    "            idx = state_to_index(state)\n",
    "            full_idx = idx + (action,)\n",
    "            counts[full_idx] += 1\n",
    "            Q[full_idx] += (G - Q[full_idx]) / counts[full_idx]\n",
    "\n",
    "    current_epsilon = max(epsilon_end, current_epsilon - epsilon_decay)\n",
    "\n",
    "    # Track training avg every 100 episodes (includes exploration)\n",
    "    if episode % 100 == 0:\n",
    "        ep_returns = [sum(r for _, _, r in traj) for traj in [trajectory]]  # Placeholder; accumulate if needed\n",
    "        train_avg_returns.append(np.mean(episode_returns[-100:] if episode > 100 else episode_returns))\n",
    "\n",
    "    # Greedy evaluation\n",
    "    if episode % eval_every == 0:\n",
    "        eval_episode_returns = []\n",
    "        for _ in range(eval_episodes):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            ep_return = 0\n",
    "            while not done:\n",
    "                action = epsilon_greedy_policy(state, 0)  # Greedy\n",
    "                state, reward, done, _, _ = env.step(action)\n",
    "                ep_return += reward\n",
    "            eval_episode_returns.append(ep_return)\n",
    "        eval_returns.append(np.mean(eval_episode_returns))\n",
    "        print(f\"Episode {episode}: Eval Avg Return = {eval_returns[-1]:.3f}\")\n",
    "\n",
    "# Plot\n",
    "episodes = np.arange(100, num_episodes + 1, 100)\n",
    "plt.plot(episodes, train_avg_returns, label='Training (with ε)')\n",
    "eval_episodes_axis = np.arange(eval_every, num_episodes + 1, eval_every)\n",
    "plt.plot(eval_episodes_axis, eval_returns, label='Greedy Eval')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Return')\n",
    "plt.title('MC Control on Blackjack')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Example policy (no ace, player 12-21, dealer 1-10)\n",
    "policy_no_ace = np.argmax(Q[12:22, 1:11, 0], axis=2)\n",
    "print(\"Policy (no ace, rows: player 12-21, cols: dealer 1-10):\")\n",
    "print(policy_no_ace)  # 0: stick, 1: hit"
   ],
   "id": "3e6cb594eb58264",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code learns a near-optimal policy (e.g., hit below ~17, stick above), with the plot showing improving returns.\n",
    "\n",
    "### 13.6.2  Temporal-Difference Learning Basics\n",
    "\n",
    "Temporal-Difference (TD) learning updates value estimates incrementally after each step, without waiting for episode ends. The core idea is bootstrapping: using current estimates to update others, blending MC's empirical averaging with DP's recursion.\n",
    "\n",
    "TD(0) evaluation for $  V^\\pi(s)  $ uses the update:\n",
    "\n",
    "$$   V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)],   $$\n",
    "\n",
    "where $  r + \\gamma V(s')  $ is the TD target, and $  \\alpha  $ is the learning rate. This reduces variance compared to MC (by bootstrapping) but introduces bias (from imperfect estimates). Convergence requires decreasing $  \\alpha  $ and sufficient exploration under the policy.\n",
    "\n",
    "A brief mention of eligibility traces: In TD(λ), traces credit recent states/actions with decaying eligibility $  e(s) = \\gamma \\lambda e(s) + 1  $, extending updates backward for faster propagation in long-horizon tasks like orbit control. TD shines in continuing space environments, enabling online learning amid ongoing disturbances.\n",
    "\n",
    "### 13.6.3  On-Policy TD Control: SARSA\n",
    "\n",
    "SARSA (State-Action-Reward-State-Action) is an on-policy TD control method that learns the action-value function $  Q^\\pi(s,a)  $ directly. It generates data using the current policy (e.g., ε-greedy) and updates based on the next action from that policy, making it suitable for learning safe, exploratory behaviors.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "- Step 1: Initialize $  Q(s,a) = 0  $ for all $  s,a  $.\n",
    "- Step 2: For each episode:\n",
    "- $~$ Start with state $  s  $, choose $  a  $ from policy (e.g., ε-greedy on $  Q  $).\n",
    "- $~$ While not terminal:\n",
    "- $~$ $~$ Take $  a  $, observe $  r, s'  $.\n",
    "- $~$ $~$ Choose next action $  a'  $ from policy.\n",
    "- $~$ $~$ Update: $  Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma Q(s',a') - Q(s,a)]  $.\n",
    "- $~$ $~$ Set $  s \\leftarrow s'  $, $  a \\leftarrow a'  $.\n",
    "- Step 3: Decay ε over time for greedier policy.\n",
    "\n",
    "\n",
    "The update rule reflects the policy's behavior, leading to \"cautious\" optima (e.g., avoiding risky shortcuts). In space, SARSA suits on-policy adaptation, like thruster control under noise.\n",
    "\n",
    "Below is a Python example of tabular SARSA on Gymnasium's CliffWalking environment (4x12 grid; start at (3,0), goal (3,11); cliff at (3,1-10) gives -100 and reset; actions: up/down/left/right).\n"
   ],
   "id": "faafd7be058506d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch, Rectangle\n",
    "\n",
    "env = gym.make('CliffWalking-v1')\n",
    "num_episodes = 500\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.1\n",
    "\n",
    "# Q-table: 48 states (4x12 grid), 4 actions (0:up, 1:right, 2:down, 3:left)\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    action = np.argmax(Q[state]) if np.random.rand() > epsilon else env.action_space.sample()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_action = np.argmax(Q[next_state]) if np.random.rand() > epsilon else env.action_space.sample()\n",
    "        Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "        state, action = next_state, next_action\n",
    "        total_reward += reward\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('SARSA Learning Curve on CliffWalking')\n",
    "plt.show()\n",
    "\n",
    "# Extract policy and values (max Q per state)\n",
    "policy = np.argmax(Q, axis=1)\n",
    "values = np.max(Q, axis=1)\n",
    "\n",
    "# Visualization function\n",
    "def visualize_cliff_policy(policy, values):\n",
    "    rows, cols = 4, 12\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.set_xlim(0, cols)\n",
    "    ax.set_ylim(0, rows)\n",
    "    ax.set_xticks(np.arange(cols + 1))\n",
    "    ax.set_yticks(np.arange(rows + 1))\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Arrow directions: up, right, down, left\n",
    "    arrows = [(0, 0.3), (0.3, 0), (0, -0.3), (-0.3, 0)]\n",
    "    labels = ['↑', '→', '↓', '←']\n",
    "\n",
    "    # Start at (3,0), goal at (3,11), cliff (3,1) to (3,10)\n",
    "    start = 36  # 3*12 + 0\n",
    "    goal = 47   # 3*12 + 11\n",
    "    cliff_start = 37  # 3*12 + 1\n",
    "    cliff_end = 46    # 3*12 + 10\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            s = i * cols + j\n",
    "            x, y = j + 0.5, rows - 0.5 - i  # Center, flip y for top-left row 0\n",
    "\n",
    "            # Background colors\n",
    "            color = 'white'\n",
    "            if s == start:\n",
    "                color = 'lightgreen'\n",
    "            elif s == goal:\n",
    "                color = 'gold'\n",
    "            elif cliff_start <= s <= cliff_end:\n",
    "                color = 'lightcoral'\n",
    "            ax.add_patch(Rectangle((j, rows - 1 - i), 1, 1, color=color, alpha=0.5))\n",
    "\n",
    "            if s == goal or (cliff_start <= s <= cliff_end):  # No policy on terminals/cliff\n",
    "                ax.text(x, y, f'V={values[s]:.1f}' if s != goal else 'Goal', ha='center', va='center', fontsize=8)\n",
    "                continue\n",
    "\n",
    "            # Policy arrow\n",
    "            a = policy[s]\n",
    "            dx, dy = arrows[a]\n",
    "            ax.add_patch(FancyArrowPatch((x - dx/2, y - dy/2), (x + dx/2, y + dy/2),\n",
    "                                         arrowstyle='->', mutation_scale=15, color='blue'))\n",
    "\n",
    "            # Value label\n",
    "            ax.text(x, y - 0.3, f'V={values[s]:.1f}', ha='center', va='center', fontsize=6)\n",
    "\n",
    "    ax.set_title('SARSA Learned Policy and Values on CliffWalking\\n(Green: Start, Gold: Goal, Red: Cliff)')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "visualize_cliff_policy(policy, values)"
   ],
   "id": "6579d98c5e26b51a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This learns a safe path along the top, avoiding the cliff.",
   "id": "ba2c00f33574c2db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13.6.4  Off-Policy TD Control: Q-Learning\n",
    "\n",
    "Q-Learning is an off-policy TD control method that learns the optimal $  Q^*(s,a)  $ while exploring with a separate behavior policy (e.g., ε-greedy). It uses importance sampling implicitly via the max operator, allowing data reuse and decoupling exploration from the target greedy policy.\n",
    "\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Initialize $Q(s, a) = 0 $.\n",
    "\n",
    "2. For each step:\n",
    "\n",
    "   - From $ s $, choose $ a $ from behavior policy.\n",
    "\n",
    "   - Observe $ r, s' $.\n",
    "\n",
    "   - Update: $ Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)] $.\n",
    "\n",
    "3. The max assumes the optimal policy, converging to $ Q^* $regardless of behavior (as long as all pairs are visited infinitely).\n",
    "The max assumes the optimal policy, converging to $  Q^*  $ regardless of behavior (as long as all pairs are visited infinitely).\n",
    "\n",
    "Importance sampling corrects for policy differences in off-policy learning generally, but Q-Learning avoids explicit ratios by targeting the optimal. In space, it's ideal for sim-based training with aggressive exploration, transferring to conservative real policies.\n",
    "Below is a Python example of tabular Q-Learning on Gymnasium's Taxi environment (5x5 grid; 4 passenger locations, 4 destinations; actions: move N/S/E/W, pickup/dropoff)."
   ],
   "id": "925195d4868974c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "num_episodes = 1000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Q-table: 500 states, 6 actions\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state]) if np.random.rand() > epsilon else env.action_space.sample()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning on Taxi')\n",
    "plt.show()\n",
    "\n",
    "# Policy\n",
    "policy = np.argmax(Q, axis=1)\n",
    "print(\"Optimal Policy Snippet (first 10 states):\", policy[:10])"
   ],
   "id": "a5fd60cafd20a8aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This learns efficient taxi navigation, with rewards improving to ~8-10 per episode.",
   "id": "41a341cdfbdaf8d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.7  Function Approximation and Deep Reinforcement Learning\n",
    "\n",
    "Tabular methods like those in previous sections work well for small, discrete state-action spaces but fail in high-dimensional or continuous environments common in engineering, such as spacecraft dynamics with continuous states (position, velocity, attitude). Function approximation addresses this by parameterizing value functions or policies with models like linear regressors or neural networks, enabling generalization across similar states. This scales RL to real-world problems, evolving from simple linear approximations (e.g., tile coding) to deep neural networks that learn hierarchical features from raw inputs like sensor data. In space engineering, this allows handling complex, non-linear systems like orbital perturbations without exhaustive tabulation.\n",
    "\n",
    "### 13.7.1  Need for Function Approximation\n",
    "\n",
    "The **curse of dimensionality** plagues tabular RL: As state dimensions grow (e.g., 6-DOF spacecraft pose plus velocities), the number of states explodes exponentially, making storage and exploration infeasible. For instance, discretizing each dimension into 10 bins yields $  10^d  $ states for $  d  $ dimensions—unmanageable for $  d > 10  $.\n",
    "\n",
    "Function approximation mitigates this by representing values as parameterized functions, e.g., $  \\hat{V}(s; \\theta) \\approx V^\\pi(s)  $ or $  \\hat{Q}(s,a; \\theta) \\approx Q^\\pi(s,a)  $, where $  \\theta  $ are weights updated via gradient descent on errors like TD residuals. Linear approximators (e.g., $  \\hat{V}(s) = \\theta^\\top \\phi(s)  $, with features $  \\phi  $) offer guarantees in some cases, but non-linear models like neural nets handle complex mappings. This enables RL in continuous spaces, crucial for engineering tasks like optimal control under uncertainty.\n",
    "\n",
    "### 13.7.2  Deep Q-Networks (DQN)\n",
    "\n",
    "Deep Q-Networks (DQN) extend Q-learning to high-dimensional inputs using deep neural networks as Q-approximators, $  Q(s,a; \\theta)  $. The architecture typically includes convolutional layers for image inputs (e.g., from cameras) or fully connected layers for vector states, outputting Q-values for each action.\n",
    "\n",
    "Key innovations include experience replay: A buffer stores transitions $  (s, a, r, s')  $, sampled in mini-batches to break correlations and stabilize training. Target networks decouple updates: A separate network $  Q(s',a'; \\theta^-)  $ computes targets, updated slowly (e.g., soft: $  \\theta^- \\leftarrow \\tau \\theta + (1-\\tau) \\theta^-  $) to reduce oscillations.\n",
    "DQN optimizes via SGD on the loss $  \\mathcal{L} = [r + \\gamma \\max_{a'} Q(s',a'; \\theta^-) - Q(s,a; \\theta)]^2  $, with ε-greedy exploration.\n",
    "\n",
    "> #### Example: DQN on CartPole\n",
    "> Below is a Python example of a simple DQN using PyTorch on Gymnasium's CartPole-v1 (continuous state: cart position/velocity, pole angle/rate; discrete actions: left/right). It trains an agent to balance the pole.\n",
    "\n",
    "\n"
   ],
   "id": "b31dc6f2127a815b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]  # 4\n",
    "action_dim = env.action_space.n  # 2\n",
    "hidden_dim = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "target_update = 10  # episodes\n",
    "buffer_size = 10000\n",
    "num_episodes = 500\n",
    "\n",
    "# Neural Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Replay Buffer\n",
    "replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "# Networks\n",
    "policy_net = DQN(state_dim, action_dim, hidden_dim)\n",
    "target_net = DQN(state_dim, action_dim, hidden_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(state)\n",
    "    return q_values.argmax().item()\n",
    "\n",
    "# Training\n",
    "episode_rewards = []\n",
    "epsilon = epsilon_start\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = torch.FloatTensor(np.array(states))\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "            rewards = torch.FloatTensor(rewards)\n",
    "            next_states = torch.FloatTensor(np.array(next_states))\n",
    "            dones = torch.FloatTensor(dones)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions).squeeze()\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0]\n",
    "            targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = criterion(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN on CartPole')\n",
    "plt.show()"
   ],
   "id": "65ff386d52b7224d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML  # For displaying animation in Jupyter\n",
    "\n",
    "# Hyperparameters\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')  # Enable rgb_array for frames\n",
    "state_dim = env.observation_space.shape[0]  # 4\n",
    "action_dim = env.action_space.n  # 2\n",
    "hidden_dim = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "target_update = 10  # episodes\n",
    "buffer_size = 10000\n",
    "num_episodes = 500\n",
    "\n",
    "# Neural Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Replay Buffer\n",
    "replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "# Networks\n",
    "policy_net = DQN(state_dim, action_dim, hidden_dim)\n",
    "target_net = DQN(state_dim, action_dim, hidden_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(state)\n",
    "    return q_values.argmax().item()\n",
    "\n",
    "# Training\n",
    "episode_rewards = []\n",
    "epsilon = epsilon_start\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = torch.FloatTensor(np.array(states))\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "            rewards = torch.FloatTensor(rewards)\n",
    "            next_states = torch.FloatTensor(np.array(next_states))\n",
    "            dones = torch.FloatTensor(dones)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions).squeeze()\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1)[0]\n",
    "            targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = criterion(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN on CartPole')\n",
    "plt.show()\n",
    "\n",
    "# Evaluation with visualization\n",
    "def evaluate_and_visualize(num_steps=200):\n",
    "    state, _ = env.reset()\n",
    "    frames = []  # Collect RGB frames\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < num_steps:\n",
    "        frame = env.render()  # Get RGB array\n",
    "        frames.append(frame)\n",
    "        action = select_action(state, 0)  # Greedy\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "    env.close()\n",
    "\n",
    "    # Animate frames\n",
    "    fig = plt.figure()\n",
    "    img = plt.imshow(frames[0])\n",
    "    def animate(i):\n",
    "        img.set_array(frames[i])\n",
    "        return [img]\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50)\n",
    "    plt.close()  # Close to prevent static display\n",
    "    return anim\n",
    "\n",
    "# Display animation in Jupyter (requires %matplotlib inline or notebook)\n",
    "anim = evaluate_and_visualize()\n",
    "HTML(anim.to_jshtml())"
   ],
   "id": "81cab3f9c3ebadb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 13.7.3  Stability and Extensions\n",
    "\n",
    "DQN's basic form can suffer from overestimation bias (max operator amplifies noise) and instability. Double DQN addresses bias by decoupling action selection and evaluation: Target uses policy net for argmax, target net for value: reducing overoptimism in uncertain space environments.\n",
    "\n",
    "Dueling DQN factorizes Q into state-value $  V(s)  $ and advantage $  A(s,a)  $: $  Q(s,a) = V(s) + (A(s,a) - \\frac{1}{|A|} \\sum_{a'} A(s,a'))  $, with separate streams in the network. This improves learning by isolating state quality from action benefits, aiding tasks like multi-axis satellite control with shared state values. These extensions enhance robustness for engineering deployments."
   ],
   "id": "2d9623a99fb74ccf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.8  Policy Gradient Methods and Actor-Critic\n",
    "\n",
    "Policy gradient methods shift from value-based approaches (like Q-learning) to direct policy optimization, parameterizing the policy $  \\pi_\\theta(a|s)  $ (e.g., as a neural network with weights $  \\theta  $) and updating $  \\theta  $ to maximize expected returns. This is advantageous in continuous action spaces, common in engineering, where discretizing actions is impractical (e.g., variable thrust in satellite control). The foundational REINFORCE algorithm uses Monte-Carlo estimates of gradients, but suffers from high variance due to full-episode sampling. Baselines (e.g., subtracting a state-value estimate) reduce variance without introducing bias, improving stability. Actor-critic methods combine policy gradients (the \"actor\") with value estimation (the \"critic\"), enabling bootstrapping for lower variance and faster learning: key for real-time adaptation in space systems like non-linear orbit maneuvers under perturbations.\n",
    "\n",
    "In the course nomenclature, the state $  s  $ aligns with the dynamic state vector $  \\mathbf{y}(t)  $ (e.g., position and velocity), while actions $  a  $ correspond to control inputs $  \\mathbf{u}  $ (e.g., thrust), and the dynamics relate to $  \\dot{\\mathbf{y}} = \\mathbf{f}(\\mathbf{y}, \\mathbf{u})  $, with parameters $  \\mathbf{p}  $ capturing uncertainties like drag.\n",
    "\n",
    "### 13.8.1  Policy Gradient Theorem\n",
    "\n",
    "The policy gradient theorem provides a mathematical foundation for directly computing gradients of the performance objective $  J(\\theta) = \\mathbb{E}_\\pi [G_0]  $, the expected return under policy $  \\pi_\\theta  $.\n",
    "\n",
    "At a high level, the derivation starts from the MDP framework, expressing $  J(\\theta)  $ via the stationary distribution $  d^\\pi(s)  $ (probability of visiting $  s  $ under $  \\pi  $):\n",
    "$   J(\\theta) = \\sum_s d^\\pi(s) \\sum_a \\pi_\\theta(a|s) Q^\\pi(s,a).   $\n",
    "\n",
    "Differentiating with respect to $  \\theta  $ (using the log-trick $  \\nabla_\\theta \\pi = \\pi \\nabla_\\theta \\log \\pi  $) yields:\n",
    "\n",
    "$   \\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a) \\right],   $\n",
    "\n",
    "where the expectation is over states and actions sampled from trajectories under $  \\pi_\\theta  $. Replacing $  Q^\\pi  $ with sampled returns $  G_t  $ (Monte-Carlo) or advantages (to reduce variance) enables stochastic gradient ascent: $  \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)  $. This theorem ensures unbiased gradients, even in stochastic policies, making it suitable for fine-grained control in engineering, like optimizing probabilistic thrust profiles for fuel efficiency.\n",
    "\n",
    "Relating to course nomenclature, $  s  $ here is akin to $  \\mathbf{y}  $, $  a  $ to $  \\mathbf{u}  $, and the objective ties to optimizing over feasible sets $  \\mathcal{Y}  $ and parameters $  \\mathbf{p}  $.\n",
    "\n",
    "### 13.8.2  REINFORCE Algorithm\n",
    "\n",
    "REINFORCE (REward Increment = Nonnegative Factor times Offset Reinforcement and Characteristic Eligibility) is a Monte-Carlo policy gradient method that samples full episodes to estimate $  \\nabla_\\theta J(\\theta)  $, updating the policy via:\n",
    "$   \\theta \\leftarrow \\theta + \\alpha G_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t),   $\n",
    "\n",
    "summing over timesteps $  t  $ (with discounting). It directly optimizes stochastic policies, ideal for exploration in continuous spaces.\n",
    "\n",
    "However, variance issues arise from noisy return estimates $  G_t  $, especially in long-horizon tasks—leading to slow convergence or instability. Baselines (e.g., subtracting $  V(s_t)  $) mitigate this: $  G_t - V(s_t)  $, centering gradients around zero without bias. In space engineering, REINFORCE suits episodic simulations like trajectory optimization, but variance challenges real-time use.\n",
    "\n",
    "Below is a Python example of vanilla REINFORCE (with baseline) using PyTorch on Gymnasium's Pendulum-v1 (continuous action: torque $  [-2,2]  $; state: angle/velocity; goal: upright balance). The state relates to $  \\mathbf{y}  $, action to scalar $  u  $.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "863bfec9e1baa3a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters (tuned for better convergence)\n",
    "env = gym.make('Pendulum-v1')\n",
    "state_dim = env.observation_space.shape[0]  # 3\n",
    "action_dim = 1  # Continuous torque\n",
    "hidden_dim = 128  # Increased\n",
    "lr_actor = 3e-4   # Lowered\n",
    "lr_critic = 1e-3  # Lowered\n",
    "gamma = 0.99\n",
    "num_episodes = 2000  # Increased\n",
    "max_steps = 300      # Increased\n",
    "entropy_beta = 0.01  # For entropy regularization\n",
    "\n",
    "# Policy Network (Actor) with added layer\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # Added layer\n",
    "        self.fc_mu = nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))  # Added\n",
    "        mu = 2 * torch.tanh(self.fc_mu(x))  # Scale to [-2,2]\n",
    "        sigma = F.softplus(self.fc_sigma(x)) + 1e-5\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample(self, state):\n",
    "        mu, sigma = self(state)\n",
    "        dist = Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action.item(), log_prob, entropy\n",
    "\n",
    "# Value Network (Baseline) with added layer\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # Added layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))  # Added\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize with AdamW for better generalization\n",
    "policy_net = PolicyNet(state_dim, action_dim, hidden_dim)\n",
    "value_net = ValueNet(state_dim, hidden_dim)\n",
    "actor_optim = optim.AdamW(policy_net.parameters(), lr=lr_actor)\n",
    "critic_optim = optim.AdamW(value_net.parameters(), lr=lr_critic)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "episode_rewards = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "avg_entropies = []\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    entropies = []  # New for monitoring\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < max_steps:\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action, log_prob, entropy = policy_net.sample(state_tensor)  # Updated to return entropy\n",
    "        value = value_net(state_tensor)\n",
    "        next_state, reward, done, _, _ = env.step([action])\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "    # Compute returns and advantages\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantages = returns - values.detach()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # Normalize\n",
    "\n",
    "    # Actor loss with entropy\n",
    "    actor_loss = -torch.sum(torch.stack(log_probs) * advantages) - entropy_beta * torch.mean(torch.stack(entropies))\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "\n",
    "    # Critic loss\n",
    "    critic_loss = criterion(values, returns)\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "\n",
    "    episode_rewards.append(sum(rewards))\n",
    "    actor_losses.append(actor_loss.item())\n",
    "    critic_losses.append(critic_loss.item())\n",
    "    avg_entropies.append(torch.mean(torch.stack(entropies)).item())\n",
    "\n",
    "# Plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].plot(episode_rewards); axs[0, 0].set_title('Episode Rewards')\n",
    "axs[0, 1].plot(avg_entropies); axs[0, 1].set_title('Average Entropy')\n",
    "axs[1, 0].plot(actor_losses); axs[1, 0].set_title('Actor Losses')\n",
    "axs[1, 1].plot(critic_losses); axs[1, 1].set_title('Critic Losses')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "26d433ac42b90e10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters (tuned for better convergence)\n",
    "env = gym.make('Pendulum-v1')\n",
    "state_dim = env.observation_space.shape[0]  # 3\n",
    "action_dim = 1  # Continuous torque\n",
    "hidden_dim = 128  # Increased\n",
    "lr_actor = 3e-1   # Lowered\n",
    "lr_critic = 2e-1  # Lowered\n",
    "gamma = 0.99\n",
    "num_episodes = 2000  # Increased\n",
    "max_steps = 300      # Increased\n",
    "entropy_beta = 0.01  # For entropy regularization\n",
    "\n",
    "# Policy Network (Actor) with added layer\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # Added layer\n",
    "        self.fc_mu = nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))  # Added\n",
    "        mu = 2 * torch.tanh(self.fc_mu(x))  # Scale to [-2,2]\n",
    "        sigma = F.softplus(self.fc_sigma(x)) + 1e-5\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample(self, state):\n",
    "        mu, sigma = self(state)\n",
    "        dist = Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action.item(), log_prob, entropy\n",
    "\n",
    "# Value Network (Baseline) with added layer\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # Added layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))  # Added\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize with AdamW for better generalization\n",
    "policy_net = PolicyNet(state_dim, action_dim, hidden_dim)\n",
    "value_net = ValueNet(state_dim, hidden_dim)\n",
    "actor_optim = optim.AdamW(policy_net.parameters(), lr=lr_actor)\n",
    "critic_optim = optim.AdamW(value_net.parameters(), lr=lr_critic)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "episode_rewards = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "avg_entropies = []\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    entropies = []  # New for monitoring\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < max_steps:\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action, log_prob, entropy = policy_net.sample(state_tensor)  # Updated to return entropy\n",
    "        value = value_net(state_tensor)\n",
    "        next_state, reward, done, _, _ = env.step([action])\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "    # Compute returns and advantages\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantages = returns - values.detach()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # Normalize\n",
    "\n",
    "    # Actor loss with entropy\n",
    "    actor_loss = -torch.sum(torch.stack(log_probs) * advantages) - entropy_beta * torch.mean(torch.stack(entropies))\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "\n",
    "    # Critic loss\n",
    "    critic_loss = criterion(values, returns)\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "\n",
    "    episode_rewards.append(sum(rewards))\n",
    "    actor_losses.append(actor_loss.item())\n",
    "    critic_losses.append(critic_loss.item())\n",
    "    avg_entropies.append(torch.mean(torch.stack(entropies)).item())\n",
    "\n",
    "# Plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].plot(episode_rewards); axs[0, 0].set_title('Episode Rewards')\n",
    "axs[0, 1].plot(avg_entropies); axs[0, 1].set_title('Average Entropy')\n",
    "axs[1, 0].plot(actor_losses); axs[1, 0].set_title('Actor Losses')\n",
    "axs[1, 1].plot(critic_losses); axs[1, 1].set_title('Critic Losses')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "220416173f9e9276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env = gym.make('Pendulum-v1')\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Hyperparameters (further tuned)\n",
    "state_dim = env.observation_space.shape[0]  # 3\n",
    "action_dim = 1  # Continuous torque\n",
    "hidden_dim = 128\n",
    "lr_actor = 3e-4\n",
    "lr_critic = 1e-3\n",
    "gamma = 0.99\n",
    "num_episodes = 3000  # Increased further\n",
    "max_steps = 300\n",
    "entropy_beta = 0.01\n",
    "clip_grad_norm = 1.0  # New: Gradient clipping value\n",
    "num_envs = 4  # New: Multi-env rollouts for variance reduction\n",
    "\n",
    "# Policy Network (Actor)\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mu = 2 * torch.tanh(self.fc_mu(x))  # Scale to [-2,2]\n",
    "        sigma = F.softplus(self.fc_sigma(x)) + 1e-5\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample(self, state):\n",
    "        mu, sigma = self(state)\n",
    "        dist = Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action.item(), log_prob, entropy\n",
    "\n",
    "# Value Network (Baseline)\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize\n",
    "policy_net = PolicyNet(state_dim, action_dim, hidden_dim)\n",
    "value_net = ValueNet(state_dim, hidden_dim)\n",
    "actor_optim = optim.AdamW(policy_net.parameters(), lr=lr_actor)\n",
    "critic_optim = optim.AdamW(value_net.parameters(), lr=lr_critic)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training with multi-env rollouts\n",
    "episode_rewards = []\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "avg_entropies = []\n",
    "for episode in range(num_episodes):\n",
    "    # Collect multiple trajectories\n",
    "    all_log_probs = []\n",
    "    all_rewards = []\n",
    "    all_values = []\n",
    "    all_entropies = []\n",
    "    batch_rewards = 0\n",
    "    for _ in range(num_envs):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        entropies = []\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < max_steps:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action, log_prob, entropy = policy_net.sample(state_tensor)\n",
    "            value = value_net(state_tensor)\n",
    "            next_state, reward, done, _, _ = env.step([action])\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            entropies.append(entropy)\n",
    "\n",
    "            state = next_state\n",
    "            step += 1\n",
    "\n",
    "        all_log_probs.extend(log_probs)\n",
    "        all_rewards.extend(rewards)\n",
    "        all_values.extend(values)\n",
    "        all_entropies.extend(entropies)\n",
    "        batch_rewards += sum(rewards)\n",
    "\n",
    "    # Compute returns and advantages (across batch)\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(all_rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    values = torch.cat(all_values).squeeze()\n",
    "    advantages = returns - values.detach()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # Normalize\n",
    "\n",
    "    # Actor loss with entropy\n",
    "    actor_loss = -torch.sum(torch.stack(all_log_probs) * advantages) - entropy_beta * torch.mean(torch.stack(all_entropies))\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), clip_grad_norm)  # Clip gradients\n",
    "    actor_optim.step()\n",
    "\n",
    "    # Critic loss\n",
    "    critic_loss = criterion(values, returns)\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(value_net.parameters(), clip_grad_norm)  # Clip gradients\n",
    "    critic_optim.step()\n",
    "\n",
    "    episode_rewards.append(batch_rewards / num_envs)  # Average per env\n",
    "    actor_losses.append(actor_loss.item())\n",
    "    critic_losses.append(critic_loss.item())\n",
    "    avg_entropies.append(torch.mean(torch.stack(all_entropies)).item())\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}: Avg Reward = {episode_rewards[-1]:.2f}, Avg Entropy = {avg_entropies[-1]:.2f}\")\n",
    "\n",
    "# Plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].plot(episode_rewards); axs[0, 0].set_title('Episode Rewards')\n",
    "axs[0, 1].plot(avg_entropies); axs[0, 1].set_title('Average Entropy')\n",
    "axs[1, 0].plot(actor_losses); axs[1, 0].set_title('Actor Losses')\n",
    "axs[1, 1].plot(critic_losses); axs[1, 1].set_title('Critic Losses')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3cad3fbc94006931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "env_id = \"Pendulum-v1\"\n",
    "num_envs = 4\n",
    "vec_env = make_vec_env(env_id, n_envs=num_envs)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1, n_steps=2048, batch_size=64, n_epochs=10, learning_rate=3e-4)\n",
    "\n",
    "# Training\n",
    "model.learn(total_timesteps=300000)  # ~3000 effective episodes\n",
    "\n",
    "# Evaluation\n",
    "episode_rewards = []\n",
    "for episode in range(10):\n",
    "    obs = vec_env.reset()\n",
    "    done = [False] * num_envs\n",
    "    total_reward = [0] * num_envs\n",
    "    while not all(done):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = vec_env.step(action)\n",
    "        for i in range(num_envs):\n",
    "            total_reward[i] += reward[i]\n",
    "    episode_rewards.append(np.mean(total_reward))\n",
    "\n",
    "print(\"Mean Eval Reward:\", np.mean(episode_rewards))\n",
    "\n",
    "# Plot (for training, use tensorboard if set up)\n",
    "plt.plot(episode_rewards)  # Eval only\n",
    "plt.title('PPO Eval Rewards on Pendulum')\n",
    "plt.show()"
   ],
   "id": "1ff8d777bdf5d330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Hyperparameters (tuned for better performance)\n",
    "env_id = \"Pendulum-v1\"\n",
    "num_envs = 4\n",
    "vec_env = make_vec_env(env_id, n_envs=num_envs)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1, n_steps=2048, batch_size=128, n_epochs=20, learning_rate=1e-3, clip_range=0.2)\n",
    "\n",
    "# Training\n",
    "model.learn(total_timesteps=500000)  # Increased\n",
    "\n",
    "# Evaluation\n",
    "mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10)\n",
    "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Animation for one episode using a single env\n",
    "single_env = gym.make(env_id, render_mode='rgb_array')\n",
    "obs, _ = single_env.reset()\n",
    "frames = []\n",
    "done = False\n",
    "step = 0\n",
    "max_steps = 300  # Limit for visualization\n",
    "while not done and step < max_steps:\n",
    "    frame = single_env.render()\n",
    "    frames.append(frame)\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, _, done, _, _ = single_env.step(action)\n",
    "    step += 1\n",
    "single_env.close()\n",
    "\n",
    "# Display animation\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "img = plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "def animate(i):\n",
    "    img.set_array(frames[i])\n",
    "    return [img]\n",
    "anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_jshtml())"
   ],
   "id": "d3cec9fbe438c6e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code learns to swing up and balance the pendulum, with rewards improving from ~ -1500 to ~ -100.\n",
    "\n",
    "### 13.8.3  Actor-Critic Frameworks\n",
    "Actor-critic methods separate the policy (actor) from value estimation (critic), using the critic to compute advantages for lower-variance gradients. Advantage Actor-Critic (A2C) synchronously collects data from multiple environments, updating the actor with $  \\nabla_\\theta \\log \\pi_\\theta(a|s) \\hat{A}(s,a)  $ (where $  \\hat{A} = Q(s,a) - V(s) \\approx r + \\gamma V(s') - V(s)  $) and the critic via TD errors. This balances bias-variance, enabling stable learning in continuous control.\n",
    "\n",
    "A brief on Asynchronous A3C: Extends A2C with parallel actors on separate threads, asynchronously updating a shared model—improving exploration and speed, though with some staleness. In space applications, actor-critic suits hybrid discrete-continuous tasks like rover steering with energy constraints.\n",
    "\n",
    "> #### Example: A2C on MountainCar-v0\n",
    "> Below is a Python example of simple A2C (synchronous actor-critic) using PyTorch on Gymnasium's MountainCar-v0 (discrete actions: left/none/right; continuous state: position/velocity; goal: reach flag). State relates to $  \\mathbf{y}  $, actions to discrete $  u  $.\n"
   ],
   "id": "aadb1a34ec9b0ffb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import animation\n",
    "\n",
    "# Hyperparameters\n",
    "env = gym.make('MountainCar-v0')\n",
    "state_dim = env.observation_space.shape[0]  # 2\n",
    "action_dim = env.action_space.n  # 3\n",
    "hidden_dim = 256\n",
    "lr = 7e-4\n",
    "gamma = 0.99\n",
    "lam = 0.95  # GAE lambda\n",
    "vf_coef = 0.5\n",
    "ent_coef = 0.01\n",
    "#num_episodes = 5000\n",
    "num_episodes = 8000\n",
    "max_steps = 200\n",
    "velocity_bonus = 10.0  # Coefficient for velocity-based reward shaping\n",
    "\n",
    "# Running normalization\n",
    "class RunningNorm:\n",
    "    def __init__(self, shape):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = 1e-4\n",
    "\n",
    "    def update(self, batch):\n",
    "        batch = np.asarray(batch, dtype='float64')\n",
    "        batch_mean = np.mean(batch, axis=0)\n",
    "        batch_var = np.var(batch, axis=0)\n",
    "        batch_count = batch.shape[0]\n",
    "\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count\n",
    "\n",
    "        self.var = M2 / tot_count\n",
    "        self.mean += delta * batch_count / tot_count\n",
    "        self.count = tot_count\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / np.sqrt(self.var + 1e-8)\n",
    "\n",
    "# Actor-Critic Network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        policy_logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "# Initialize\n",
    "ac_net = ActorCritic(state_dim, action_dim, hidden_dim)\n",
    "optimizer = optim.RMSprop(ac_net.parameters(), lr=lr)\n",
    "norm = RunningNorm((state_dim,))\n",
    "\n",
    "# Function to visualize with animation (for Jupyter notebook)\n",
    "def display_episode(model, env_id='MountainCar-v0', title='Episode', greedy=True, render_mode='rgb_array'):\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < max_steps * 2:\n",
    "        frames.append(env.render())\n",
    "        state_norm = norm.normalize(state)\n",
    "        state_tensor = torch.FloatTensor(state_norm)\n",
    "        policy_logits, _ = model(state_tensor)\n",
    "        if greedy:\n",
    "            action = torch.argmax(policy_logits).item()\n",
    "        else:\n",
    "            dist = torch.softmax(policy_logits, dim=0)\n",
    "            action = torch.multinomial(dist, 1).item()\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "    env.close()\n",
    "\n",
    "    if frames:\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        patch = plt.imshow(frames[0])\n",
    "\n",
    "        def animate(i):\n",
    "            patch.set_data(frames[i])\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50)\n",
    "        plt.close(fig)\n",
    "        return HTML(anim.to_jshtml())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Alternative: Render with pygame (human mode, opens window)\n",
    "def render_human(model, env_id='MountainCar-v0', greedy=True):\n",
    "    env = gym.make(env_id, render_mode='human')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < max_steps * 2:\n",
    "        state_norm = norm.normalize(state)\n",
    "        state_tensor = torch.FloatTensor(state_norm)\n",
    "        policy_logits, _ = model(state_tensor)\n",
    "        if greedy:\n",
    "            action = torch.argmax(policy_logits).item()\n",
    "        else:\n",
    "            dist = torch.softmax(policy_logits, dim=0)\n",
    "            action = torch.multinomial(dist, 1).item()\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        env.render()\n",
    "        step += 1\n",
    "    env.close()\n",
    "\n",
    "# Visualize before training\n",
    "print(\"Before Training (animation - run in Jupyter for display)\")\n",
    "anim = display_episode(ac_net, greedy=False)\n",
    "display(anim)\n",
    "# Or use: render_human(ac_net, greedy=False) to open a pygame window\n",
    "\n",
    "# Training\n",
    "episode_rewards = []\n",
    "all_states = []  # To update norm after some episodes\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    states = [state.copy()]\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    entropies = []\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < max_steps:\n",
    "        state_norm = norm.normalize(state)\n",
    "        state_tensor = torch.FloatTensor(state_norm)\n",
    "        policy_logits, value = ac_net(state_tensor)\n",
    "        dist = torch.softmax(policy_logits, dim=0)\n",
    "        action = torch.multinomial(dist, 1).item()\n",
    "        log_prob = torch.log(dist[action])\n",
    "        entropy = -torch.sum(dist * torch.log(dist + 1e-10))\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Reward shaping: add bonus for higher absolute velocity to encourage acceleration\n",
    "        reward += velocity_bonus * abs(next_state[1])\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        entropies.append(entropy)\n",
    "        states.append(next_state.copy())\n",
    "\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "    # Update normalizer\n",
    "    norm.update(np.array(states))\n",
    "\n",
    "    # Compute next_value\n",
    "    next_state_norm = norm.normalize(state)\n",
    "    _, next_value = ac_net(torch.FloatTensor(next_state_norm))\n",
    "    next_value = next_value.item()\n",
    "\n",
    "    # GAE and returns\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        if step == len(rewards) - 1:\n",
    "            next_non_terminal = 1.0 - done\n",
    "            next_v = next_value\n",
    "        else:\n",
    "            next_non_terminal = 1.0\n",
    "            next_v = values[step + 1]\n",
    "        delta = rewards[step] + gamma * next_v * next_non_terminal - values[step]\n",
    "        gae = delta + gamma * lam * next_non_terminal * gae\n",
    "        advantages.insert(0, gae)\n",
    "    advantages = torch.FloatTensor(advantages)\n",
    "\n",
    "    returns = advantages + values\n",
    "\n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # Losses\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    entropies = torch.stack(entropies)\n",
    "    actor_loss = -(log_probs * advantages).mean()\n",
    "    entropy_loss = -entropies.mean()\n",
    "    actor_loss += ent_coef * entropy_loss\n",
    "    critic_loss = nn.MSELoss()(values, returns)\n",
    "    loss = actor_loss + vf_coef * critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    episode_rewards.append(sum(rewards))\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_rewards[-1]}\")\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('A2C on MountainCar')\n",
    "plt.show()\n",
    "\n",
    "# Visualize after training\n",
    "print(\"After Training (animation - run in Jupyter for display)\")\n",
    "anim = display_episode(ac_net, greedy=True)\n",
    "display(anim)\n",
    "# Or use: render_human(ac_net, greedy=True) to open a pygame window\n",
    "\n",
    "# Fallback position plot if not in notebook\n",
    "def plot_position(model, greedy=True):\n",
    "    states = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < max_steps * 2:\n",
    "        states.append(state[0])\n",
    "        state_norm = norm.normalize(state)\n",
    "        state_tensor = torch.FloatTensor(state_norm)\n",
    "        policy_logits, _ = model(state_tensor)\n",
    "        if greedy:\n",
    "            action = torch.argmax(policy_logits).item()\n",
    "        else:\n",
    "            dist = torch.softmax(policy_logits, dim=0)\n",
    "            action = torch.multinomial(dist, 1).item()\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "    plt.plot(states)\n",
    "    plt.axhline(0.5, color='r')  # Goal\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Position')\n",
    "    plt.show()\n",
    "\n",
    "print(\"After Training Position Plot\")\n",
    "plot_position(ac_net, greedy=True)"
   ],
   "id": "1d70e48004bd805b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.9  Key Challenges and Practical Enhancements in RL\n",
    "\n",
    "Reinforcement learning (RL) faces several core challenges that limit its direct application in real-world engineering systems, as discussed in Brunton and Kutz (Chapter 11) and broader literature. These include the exploration-exploitation dilemma, where the agent must balance trying new actions to discover better strategies (exploration) with leveraging known good actions (exploitation); credit assignment, the difficulty of attributing delayed rewards to earlier actions in long-horizon tasks; and safety, ensuring the agent avoids catastrophic failures during learning. In space engineering contexts, these are amplified: exploration risks could lead to irreversible damage (e.g., satellite collision), credit assignment is critical for multi-step maneuvers like rendezvous, and safety is paramount in high-stakes environments like orbit control. Practical enhancements, such as advanced exploration, constraint-aware methods, and reward engineering, address these to make RL viable for computational engineering practice.\n",
    "\n",
    "### 13.9.1  Exploration Strategies\n",
    "\n",
    "[ε-greedy, entropy bonuses; no code]\n",
    "\n",
    "% Effective exploration is essential to avoid suboptimal policies trapped in local optima. ε-greedy strategies select the best-known action with probability 1-ε and a random action with probability ε (decaying over time), providing simple, directed exploration—useful in discrete spaces like mode selection in control systems (Chapter 10). For continuous policies, entropy bonuses add a term to the objective proportional to policy entropy H(π(·|s)), encouraging diverse actions (e.g., via -β H in actor loss, where β is a coefficient). This prevents policy collapse in stochastic networks and is particularly effective in high-dimensional engineering problems, like optimizing thrust vectors under uncertainty, ensuring broad sampling of the action space $  \\mathcal{U}  $.\n",
    "\n",
    "Effective exploration is essential to avoid suboptimal policies trapped in local optima. Basic techniques like random action selection provide initial diversity, but more sophisticated methods balance this with exploitation. ε-greedy strategies select the best-known action (e.g., $  \\arg\\max_a Q(s,a)  $) with probability 1-ε and a random action from the space $  A  $ with probability ε, where ε typically decays over time (e.g., exponentially: ε_{t+1} = ε_t * decay_rate). This ensures early broad sampling, transitioning to exploitation—useful in discrete spaces like mode selection in classical feedback control (Chapter 10), but can be inefficient in high-dimensional continuous spaces due to uniform randomness.\n",
    "\n",
    "For continuous or stochastic policies, entropy bonuses add a term to the objective proportional to policy entropy H(π(·|s)), encouraging diverse actions. For a Gaussian policy π_θ(u|y) = N(μ_θ(y), σ_θ(y)), entropy H ≈ log(σ) + const, so the loss includes -β H, where β is a coefficient (e.g., 0.01). This prevents policy collapse (σ → 0) and promotes exploration in parameter spaces, akin to regularization in non-linear optimization (Chapter 8). Other techniques include upper confidence bound (UCB), which adds optimism via uncertainty estimates (e.g., Q(s,a) + c √(log t / N(s,a))), or Thompson sampling, sampling actions from posterior beliefs—extending to Bayesian approaches.\n",
    "\n",
    "Relating to our chapters on global optimization (Chapter 9), exploration in RL mirrors the exploration-exploitation trade-off in derivative-free methods. ε-greedy resembles random search or perturbation in evolutionary strategies, where initial randomness explores the feasible set $  \\mathcal{X}  $, then focuses on promising regions. Entropy bonuses parallel diversity maintenance in genetic algorithms (e.g., mutation rates) or acquisition functions in Bayesian optimization (e.g., expected improvement balancing mean and variance). In both RL and global optimization, these prevent premature convergence to local minima, crucial for non-convex problems like non-linear programming with parameters p (Chapter 8).\n",
    "\n",
    "For a basic Python visualization, below is a demo on a multi-armed bandit problem (simplified RL with fixed s, multiple a). It compares ε-greedy (decaying ε) and a constant-entropy-like strategy (via softmax temperature τ, analogous to entropy bonus). The plot shows cumulative regret (suboptimality) over pulls, illustrating better exploration leading to lower regret.\n",
    "\n"
   ],
   "id": "e3b641ccd2231f84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Multi-armed bandit: 5 arms with true means (rewards)\n",
    "true_means = [0.1, 0.5, 0.9, 0.3, 0.7]  # Best arm: 2 (0.9)\n",
    "num_arms = len(true_means)\n",
    "num_pulls = 1000\n",
    "\n",
    "def pull_arm(arm):\n",
    "    return np.random.normal(true_means[arm], 1.0)  # Noisy rewards\n",
    "\n",
    "# ε-greedy strategy\n",
    "def epsilon_greedy(Q, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(num_arms)\n",
    "    return np.argmax(Q)\n",
    "\n",
    "# Softmax (entropy-like) strategy\n",
    "def softmax_select(Q, tau=1.0):\n",
    "    probs = np.exp(Q / tau) / np.sum(np.exp(Q / tau))\n",
    "    return np.random.choice(num_arms, p=probs)\n",
    "\n",
    "# Run simulation for a strategy\n",
    "def simulate(strategy, **kwargs):\n",
    "    Q = np.zeros(num_arms)\n",
    "    counts = np.zeros(num_arms)\n",
    "    rewards = []\n",
    "    regret = []\n",
    "    cumulative_regret = 0\n",
    "    epsilon = 0.1  # Initial for ε-greedy\n",
    "    for t in range(1, num_pulls + 1):\n",
    "        if strategy == 'epsilon_greedy':\n",
    "            arm = epsilon_greedy(Q, epsilon)\n",
    "            epsilon *= 0.995  # Decay\n",
    "        else:  # softmax\n",
    "            tau = kwargs['initial_tau'] / np.log(t + 1)  # Decaying temperature for entropy\n",
    "            arm = softmax_select(Q, tau)\n",
    "        reward = pull_arm(arm)\n",
    "        counts[arm] += 1\n",
    "        Q[arm] += (reward - Q[arm]) / counts[arm]\n",
    "        rewards.append(reward)\n",
    "        instant_regret = max(true_means) - true_means[arm]\n",
    "        cumulative_regret += instant_regret\n",
    "        regret.append(cumulative_regret)\n",
    "    return regret\n",
    "\n",
    "# Simulate and plot\n",
    "regret_eg = simulate('epsilon_greedy')\n",
    "regret_soft = simulate('softmax', initial_tau=5.0)  # High initial entropy\n",
    "\n",
    "plt.plot(regret_eg, label='ε-Greedy (Decaying ε)')\n",
    "plt.plot(regret_soft, label='Softmax (Decaying τ ~ Entropy Bonus)')\n",
    "plt.xlabel('Pulls')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.title('Exploration Strategies in Multi-Armed Bandit')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "6bf57eaf5a22ce1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13.9.2  Safety Envelopes and Constraint Handling\n",
    "\n",
    "\n",
    "\n",
    "Safety is a critical barrier in RL for engineering, where trial-and-error can cause physical harm or mission failure. Safe RL concepts incorporate constraints into learning, such as through constrained MDPs (CMDPs), where the policy optimizes returns subject to cost constraints (e.g., $  \\mathbb{E}[C] \\leq c  $, with C as safety costs like collision risk). Techniques like Lagrangian relaxation treat constraints as penalties in the objective, or use safety shields (e.g., model-based verifiers) to override unsafe actions. In space applications, this aligns with robust control (Chapter 11), ensuring policies respect feasible sets $  \\mathcal{Y}  $ (e.g., avoiding deorbiting due to excessive thrust) while handling parameters $  \\mathbf{p}  $ like uncertain disturbances.\n",
    "\n"
   ],
   "id": "3f610aee10e8dadf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 13.9.3  Reward Shaping and Curriculum Learning\n",
    "\n",
    "[Potential-based shaping; progressive tasks; **optional Python demo – reward shaping on toy env**]\n",
    "\n",
    "Reward shaping modifies the sparse MDP reward R to include dense guidance signals without changing optimal policies, using potential-based shaping: $  \\tilde{R}(s,a,s') = R(s,a,s') + \\gamma \\Phi(s') - \\Phi(s)  $, where $  \\Phi  $ is a potential function (e.g., distance to goal). This accelerates learning in long-horizon tasks by providing intermediate feedback. Curriculum learning progressively increases task difficulty, starting with easy variants (e.g., shorter horizons or simpler dynamics) and ramping up, mimicking staged simulations in engineering (Chapters 4-6). In space, these enable efficient training for complex maneuvers, like gradual introduction of perturbations $  \\mathbf{p}  $.\n",
    "\n",
    "> #### Example: GridWorld shaling\n",
    "> Below is a simple example of reward shaping on a toy GridWorld env (similar to Section 13.5), using Q-learning with/without shaping to show faster convergence. The potential $  \\Phi(s) = -\\|s - goal\\|  $ encourages goal proximity. Enhanced visualizations include policy arrows on the grid (showing learned actions) and value heatmaps (max Q per state), connecting the reward changes to policy improvements—e.g., shaped policies exhibit more directed paths."
   ],
   "id": "787cb72b58c80205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# Toy GridWorld: 5x5, start (0,0), goal (4,4), actions up/down/left/right\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (4, 4)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        x, y = state\n",
    "        if action == 0: y = min(y + 1, self.size - 1)  # up\n",
    "        elif action == 1: y = max(y - 1, 0)  # down\n",
    "        elif action == 2: x = max(x - 1, 0)  # left\n",
    "        elif action == 3: x = min(x + 1, self.size - 1)  # right\n",
    "        next_state = (x, y)\n",
    "        reward = 1 if next_state == self.goal else -1\n",
    "        done = next_state == self.goal\n",
    "        return next_state, reward, done\n",
    "\n",
    "# Q-Learning with optional shaping\n",
    "def q_learning_with_shaping(shaping=False, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    env = GridWorld()\n",
    "    Q = np.zeros((env.size, env.size, 4))  # States x,y; 4 actions\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.start\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(4)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            next_state, reward, done = env.step(state, action)\n",
    "            if shaping:\n",
    "                phi = -np.sum(np.abs(np.array(state) - np.array(env.goal)))  # Manhattan potential\n",
    "                phi_next = -np.sum(np.abs(np.array(next_state) - np.array(env.goal)))\n",
    "                reward += gamma * phi_next - phi\n",
    "            best_next = np.max(Q[next_state])\n",
    "            Q[state[0], state[1], action] += alpha * (reward + gamma * best_next - Q[state[0], state[1], action])\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards, Q\n",
    "\n",
    "# Visualization: Policy arrows and value heatmap\n",
    "def visualize_policy_and_values(Q, title, env):\n",
    "    policy = np.argmax(Q, axis=2)\n",
    "    values = np.max(Q, axis=2)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Policy grid\n",
    "    ax = axs[0]\n",
    "    ax.set_title(f'Policy: {title}')\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_xticks(np.arange(env.size + 1))\n",
    "    ax.set_yticks(np.arange(env.size + 1))\n",
    "    arrows = [(0, 0.3), (0, -0.3), (-0.3, 0), (0.3, 0)]  # up, down, left, right\n",
    "    for x in range(env.size):\n",
    "        for y in range(env.size):\n",
    "            if (x, y) == env.goal:\n",
    "                ax.text(x + 0.5, y + 0.5, 'Goal', ha='center', va='center')\n",
    "                continue\n",
    "            a = policy[x, y]\n",
    "            dx, dy = arrows[a]\n",
    "            ax.add_patch(FancyArrowPatch((x + 0.5 - dx/2, y + 0.5 - dy/2), (x + 0.5 + dx/2, y + 0.5 + dy/2),\n",
    "                                         arrowstyle='->', mutation_scale=15, color='blue'))\n",
    "\n",
    "    # Value heatmap\n",
    "    im = axs[1].imshow(values.T, cmap='viridis', origin='lower')\n",
    "    axs[1].set_title(f'Values: {title}')\n",
    "    plt.colorbar(im, ax=axs[1])\n",
    "    axs[1].set_xlabel('X')\n",
    "    axs[1].set_ylabel('Y')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Run and plot rewards\n",
    "rewards_base, Q_base = q_learning_with_shaping(shaping=False)\n",
    "rewards_shaped, Q_shaped = q_learning_with_shaping(shaping=True)\n",
    "\n",
    "plt.plot(rewards_base, label='Baseline')\n",
    "plt.plot(rewards_shaped, label='With Shaping')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Reward Shaping Demo on Toy GridWorld')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize policies and values\n",
    "env = GridWorld()  # For reference\n",
    "visualize_policy_and_values(Q_base, 'Baseline', env)\n",
    "visualize_policy_and_values(Q_shaped, 'With Shaping', env)"
   ],
   "id": "121e1aae44fd3fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 13.10  Summary & Roadmap to Chapter 14\n",
    "\n",
    "This chapter has introduced the foundations of reinforcement learning (RL), with methods categorized along key dimensions: model-based (explicit dynamics like $  \\dot{\\mathbf{y}} = \\mathbf{f}(\\mathbf{y}, \\mathbf{u})  $) vs. model-free (data-driven); gradient-based (using $  \\nabla  $) vs. gradient-free (derivative-free, tying to Chapter 9); and on-policy (data from current policy) vs. off-policy (reusable data). These build on earlier course topics, such as dynamic systems (Chapters 1-3), simulation (Chapters 4-6), and optimization (Chapters 7-9), extending to control under uncertainty (Chapters 10-12). Strengths include scalability to non-linear, high-dimensional problems like chaotic dynamics, while weaknesses involve sample inefficiency and lack of guarantees compared to classical methods like LQR. Looking ahead, Chapter 14 applies these to RL-based orbit station-keeping, where an agent optimizes thrust $  \\mathbf{u}  $ to maintain satellite position $  \\mathbf{y}  $ amid perturbations $  \\mathbf{p}  $ (e.g., drag, $  J_2  $), demonstrating real-time adaptation in space engineering.\n",
    "\n",
    "\n",
    "### 13.10.1  Method Comparison Table\n",
    "\n",
    "The table below summarizes key RL methods from the Brunton and Kutz categorization, highlighting their classifications, strengths, and weaknesses in the context of computational engineering.\n",
    "\n",
    "| Method | Categorization (Model-Based/Free, Gradient-Based/Free, On/Off-Policy) | Strengths | Weaknesses |\n",
    "|--------|------------------------------------------------------------------------|-----------|------------|\n",
    "| Dynamic Programming (DP) | Model-based, Gradient-free (value iteration), On-policy (policy iteration) | Exact optimality in known MDPs; efficient for small, discrete spaces (e.g., quantized control modes). | Requires full model knowledge; curse of dimensionality in high-DOF systems like continuum mechanics (Chapter 6). |\n",
    "| Monte-Carlo (MC) | Model-free, Gradient-free (tabular), On-policy (with ε-greedy) | Unbiased estimates from full episodes; simple for episodic tasks like simulations (Chapter 4). | High variance from long horizons; inefficient for continuing tasks or real-time control (e.g., orbit adjustments). |\n",
    "| Temporal-Difference (TD) | Model-free, Gradient-free (tabular Q/SARSA), On/Off-policy | Bootstrapping reduces variance; online learning for dynamic systems (Chapter 5 ODE/DAE). | Bias in early estimates; sensitive to hyperparameters like α in non-linear environments (Chapter 3 chaos). |\n",
    "| Deep Q-Networks (DQN) | Model-free, Gradient-based (neural approx.), Off-policy | Scales to high-dimensional states (e.g., sensor data); experience replay stabilizes training. | Overestimation bias; computationally intensive for deep nets, similar to non-linear optimization costs (Chapter 8). |\n",
    "| Policy Gradients (REINFORCE) | Model-free, Gradient-based, On-policy | Direct policy optimization for continuous actions Π; handles stochasticity. | High variance without baselines; slow convergence in long-horizon problems like multi-step maneuvers. |\n",
    "| Actor-Critic (A2C/A3C) | Model-free, Gradient-based, On-policy (A2C) | Lower variance via critic bootstrapping; parallelizable for faster training (A3C). | Still sensitive to hyperparameters; actor-critic divergence in unstable dynamics (e.g., non-linear control, Chapter 12). |\n",
    "\n",
    "### 13.10.2  Transition to Space Applications\n",
    "\n",
    "The foundations covered here: MDPs, value/policy iteration, TD methods, deep approximations, and policy gradients, provide a toolkit for tackling real engineering problems where models are incomplete or environments stochastic. In space applications, these enable agents to learn adaptive policies for tasks like attitude control (mapping observations $  \\mathbf{y}  $ to thrusts $  \\mathbf{u}  $) amid uncertainties $  \\mathbf{p}  $ (e.g., solar radiation), bridging classical methods (e.g., PID/LQR from Chapters 10-11) with data-driven robustness. Unlike deterministic simulations (Chapters 4-6), RL handles partial observability and non-linearity (Chapter 3), optimizing over feasible sets $  \\mathcal{Y}  $ without explicit programming. Chapter 14 applies this to RL-based orbit station-keeping, where an agent minimizes fuel while maintaining position against perturbations, showcasing hierarchical RL and edge compute for onboard autonomy: extending model-predictive control (Chapter 12) to learning-based paradigms."
   ],
   "id": "b7c3e80f6214f0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "79f6d0d386c975be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Further reading\n",
    "\n",
    "This section provides key references for deeper study, including textbooks, seminal papers, and resources tied to each subsection. The primary textbook for RL foundations is Sutton and Barto's Reinforcement Learning: An Introduction (2nd ed., 2018), which covers most topics comprehensively. Brunton and Kutz (Chapter 11) offers a data-driven perspective linking RL to dynamical systems. References are selected for their relevance to computational engineering, emphasizing numerical methods and applications.\n",
    "\n",
    "\n",
    "### RL applications in space engineering:\n",
    "\n",
    "- Reinforcement learning-based station keeping using relative orbital elements. Available at: https://www.sciencedirect.com/science/article/pii/S0273117725004533.\n",
    "- Reinforcement learning-based station keeping using relative orbital elements. Available at: https://www.diva-portal.org/smash/get/diva2:1966369/FULLTEXT02.\n",
    "- Deep reinforcement learning for station keeping on near rectilinear halo orbits. Available at: https://www.merl.com/publications/docs/TR2023-098.pdf.\n",
    "- Meta-Reinforcement Learning for Spacecraft Proximity Operations Guidance and Control in Cislunar Space. Available at: https://hanspeterschaub.info/PapersPrivate/Fereoli2025.pdf.\n",
    "- Autonomous Six-Degree-of-Freedom Spacecraft Docking with Rotating Targets via Reinforcement Learning. Available at: https://dspace.mit.edu/bitstream/handle/1721.1/145412/2008.03215.pdf?isAllowed=y&sequence=2.\n",
    "- Revisiting Space Mission Planning: A Reinforcement Learning-Guided Approach for Multi-Debris Rendezvous. Available at: https://arxiv.org/html/2409.16882v1.\n",
    "- An improved path planning and tracking control method for planetary exploration rovers on rough terrain. Available at: https://www.sciencedirect.com/science/article/pii/S2667379725000105.\n",
    "- Hopping path planning in uncertain environments for planetary exploration. Available at: https://link.springer.com/article/10.1186/s40648-022-00219-7.\n",
    "- Learning-Based End-to-End Path Planning for Lunar Rovers with Safety Constraints. Available at: https://pmc.ncbi.nlm.nih.gov/articles/PMC7866010.\n",
    "- Review of Autonomous Space Robotic Manipulators for On-Orbit Servicing and Active Debris Removal. Available at: https://spj.science.org/doi/10.34133/space.0291.\n",
    "- Review of machine learning in robotic grasping control in space application. Available at: https://www.sciencedirect.com/science/article/pii/S009457652400211X.\n",
    "- Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstration. Available at: https://arxiv.org/abs/1603.06348.\n",
    "\n",
    "### Section specific references:\n",
    "\n",
    "#### 13.1 Motivation: From Classical & Optimal Control to Learning-Based Control\n",
    "\n",
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press. (Chapters 1-2: Bridges control theory to RL, highlighting uncertainty handling.)\n",
    "- Bertsekas, D. P. (2019). Reinforcement Learning and Optimal Control. Athena Scientific. (Seminal textbook comparing RL to optimal control, with engineering examples like MPC vs. RL.)\n",
    "- Levine, S. (2018). \"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.\" arXiv preprint arXiv:1805.00909. (Paper relating RL to probabilistic control, relevant for space applications.)\n",
    "\n",
    "#### 13.1.1 When RL Shines in Engineering\n",
    "\n",
    "- Brunton, S. L., & Kutz, J. N. (2019). Data-Driven Science and Engineering (Chapter 11). Cambridge University Press. (Discusses RL in engineering contexts where models are incomplete.)\n",
    "- Recht, B. (2019). \"A Tour of Reinforcement Learning: The View from Continuous Control.\" Annual Review of Control, Robotics, and Autonomous Systems, 2, 253-279. (Review paper on RL in continuous control, with space-relevant examples like robotics.)\n",
    "\n",
    "#### 13.1.2 Limitations and Practical Considerations\n",
    "\n",
    "- Dulac-Arnold, G., et al. (2021). \"Challenges of Real-World Reinforcement Learning: Definitions, Benchmarks and Analysis.\" Machine Learning, 110(9), 2419-2468. (Paper on RL challenges in practice, including sample inefficiency and safety.)\n",
    "- Garcia, J., & Fernández, F. (2015). \"A Comprehensive Survey on Safe Reinforcement Learning.\" Journal of Machine Learning Research, 16(1), 1437-1480. (Survey on safety issues, linking to engineering constraints.)\n",
    "\n",
    "#### 13.2 Overview of Reinforcement Learning Methods\n",
    "\n",
    "- Brunton & Kutz (Chapter 11): Core categorization (Figure 11.3) of RL methods.\n",
    "- Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). \"Reinforcement Learning: A Survey.\" Journal of Artificial Intelligence Research, 4, 237-285. (Seminal survey paper on early RL categorizations.)\n",
    "\n",
    "#### 13.2.1 Model-Based vs. Model-Free RL\n",
    "\n",
    "- Sutton & Barto (2018): Chapters 8-9 on model-based vs. model-free.\n",
    "- Deisenroth, M. P., & Rasmussen, C. E. (2011). \"PILCO: A Model-Based Policy Search Method.\" Proceedings of the 28th International Conference on Machine Learning. (Seminal paper on model-based RL with Gaussian processes.)\n",
    "\n",
    "#### 13.2.2 Gradient-Based vs. Gradient-Free Methods\n",
    "\n",
    "- Schulman, J., et al. (2015). \"Trust Region Policy Optimization.\" Proceedings of the 32nd International Conference on Machine Learning. (Paper on gradient-based TRPO, linking to optimization.)\n",
    "- Hansen, N. (2006). \"The CMA Evolution Strategy: A Comparing Review.\" In Towards a New Evolutionary Computation (pp. 75-102). Springer. (On gradient-free evolutionary strategies.)\n",
    "\n",
    "#### 13.2.3 On-Policy vs. Off-Policy Learning\n",
    "\n",
    "- Sutton & Barto (2018): Chapter 5 on on/off-policy TD methods.\n",
    "- Precup, D., Sutton, R. S., & Singh, S. (2000). \"Eligibility Traces for Off-Policy Policy Evaluation.\" Proceedings of the 17th International Conference on Machine Learning. (Seminal on off-policy with traces.)\n",
    "\n",
    "#### 13.3 Markov Decision Processes (MDPs) – The Mathematical Framework\n",
    "\n",
    "- Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley. (Classic textbook on MDPs.)\n",
    "- Bellman, R. (1957). \"A Markovian Decision Process.\" Journal of Mathematics and Mechanics, 6(5), 679-684. (Seminal paper introducing MDPs.)\n",
    "\n",
    "#### 13.4 Policies, Value Functions, and Bellman Equations\n",
    "\n",
    "- Sutton & Barto (2018): Chapters 3-4 on policies, values, and Bellman equations.\n",
    "- Bellman, R. (1957). Dynamic Programming. Princeton University Press. (Original work on Bellman equations.)\n",
    "\n",
    "#### 13.5 Dynamic Programming – Solving Known MDPs\n",
    "\n",
    "- Bertsekas, D. P. (2012). Dynamic Programming and Optimal Control (Vol. 1, 4th ed.). Athena Scientific. (Detailed on DP methods.)\n",
    "- Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press. (Early work on policy iteration.)\n",
    "\n",
    "#### 13.6 Model-Free Methods: Monte-Carlo and Temporal-Difference Learning\n",
    "\n",
    "- Sutton & Barto (2018): Chapters 5-6 on MC and TD.\n",
    "- Watkins, C. J. C. H., & Dayan, P. (1992). \"Q-Learning.\" Machine Learning, 8(3-4), 279-292. (Seminal paper on Q-learning.)\n",
    "\n",
    "#### 13.7 Function Approximation and Deep Reinforcement Learning\n",
    "\n",
    "- Mnih, V., et al. (2015). \"Human-Level Control through Deep Reinforcement Learning.\" Nature, 518(7540), 529-533. (Seminal DQN paper.)\n",
    "- Tsitsiklis, J. N., & Van Roy, B. (1997). \"An Analysis of Temporal-Difference Learning with Function Approximation.\" IEEE Transactions on Automatic Control, 42(5), 674-690. (On function approximation convergence.)\n",
    "\n",
    "#### 13.8 Policy Gradient Methods and Actor-Critic\n",
    "\n",
    "- Sutton, R. S., et al. (1999). \"Policy Gradient Methods for Reinforcement Learning with Function Approximation.\" Advances in Neural Information Processing Systems, 12. (Seminal on policy gradients.)\n",
    "- Schulman, J., et al. (2017). \"Proximal Policy Optimization Algorithms.\" arXiv preprint arXiv:1707.06347. (PPO paper, extending actor-critic.)\n",
    "\n",
    "#### 13.9 Key Challenges and Practical Enhancements in RL\n",
    "\n",
    "- Brunton & Kutz (Chapter 11): Discusses challenges like exploration and credit assignment.\n",
    "- Amodei, D., et al. (2016). \"Concrete Problems in AI Safety.\" arXiv preprint arXiv:1606.06565. (On safety and exploration challenges.)\n",
    "\n",
    "#### 13.9.1 Exploration Strategies\n",
    "\n",
    "- Auer, P. (2002). \"Using Confidence Bounds for Exploitation-Exploration Trade-offs.\" Journal of Machine Learning Research, 3, 397-422. (On UCB exploration.)\n",
    "- Osband, I., et al. (2016). \"Deep Exploration via Bootstrapped DQN.\" Advances in Neural Information Processing Systems, 29. (On deep exploration techniques.)\n",
    "\n",
    "#### 13.9.2 Safety Envelopes and Constraint Handling\n",
    "\n",
    "- García & Fernández (2015): Survey on safe RL.\n",
    "- Achiam, J., et al. (2017). \"Constrained Policy Optimization.\" Proceedings of the 34th International Conference on Machine Learning. (On constrained MDPs.)\n",
    "\n",
    "#### 13.9.3 Reward Shaping and Curriculum Learning\n",
    "\n",
    "- Ng, A. Y., Harada, D., & Russell, S. (1999). \"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping.\" Proceedings of the 16th International Conference on Machine Learning. (Seminal on potential-based shaping.)\n",
    "- Bengio, Y., et al. (2009). \"Curriculum Learning.\" Proceedings of the 26th International Conference on Machine Learning. (Seminal on curriculum learning.)\n",
    "\n",
    "#### 13.10 Summary & Roadmap to Chapter 14\n",
    "\n",
    "- Izzo, D., et al. (2019). \"A Survey on Artificial Intelligence Trends in Spacecraft Guidance Dynamics and Control.\" Astrodynamics, 3(4), 287-299. (Previewing RL in space, linking to Chapter 14.)\n",
    "\n"
   ],
   "id": "fc45649041e7eb39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5a0ec26ceff14849",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
