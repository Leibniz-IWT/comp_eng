{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78c3bac",
   "metadata": {},
   "source": [
    "# 04 Simulation-Algebraic\n",
    "\n",
    "\n",
    "## 1. General purpose non-linear solvers\n",
    "\n",
    "###\n",
    "\n",
    "2. Nonlinear Solvers\n",
    "These solve $\\mathbf{f(x)} = \\mathbf{0}$ where $\\mathbf{f}$ is a nonlinear vector-valued function. Principles often build on linear solvers, using approximations like Jacobians for local convergence; global methods handle multiple roots or basins.\n",
    "2.1 Local Methods (Derivative-Based)\n",
    "\n",
    "Underlying Principle: Use Taylor expansion or quasi-Newton approximations to iterate toward a root, requiring good initial guesses; quadratic convergence for Newton's method.\n",
    "Examples:\n",
    "\n",
    "Newton-Raphson: Iterates $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\mathbf{J}^{-1} \\mathbf{f(x_k)}$ where $\\mathbf{J}$ is the Jacobian; requires Jacobian computation or finite differences.\n",
    "Quasi-Newton (e.g., Broyden's): Approximates Jacobian updates without full recomputation; good for expensive Jacobians.\n",
    "Levenberg-Marquardt: Hybrid trust-region for least-squares problems (damps Newton steps).\n",
    "\n",
    "\n",
    "SciPy Availability:\n",
    "\n",
    "Available: scipy.optimize.fsolve (hybrid Newton/minpack; finite-difference Jacobian), scipy.optimize.root (supports 'hybr', 'lm', 'broyden1', 'broyden2', 'krylov'), scipy.optimize.newton (scalar only, secant/Newton).\n",
    "Not Available: Built-in damped Newton variants or automatic differentiation for Jacobians (use JAX or autograd externally); no native homotopy continuation (e.g., for path-tracking multiple roots).\n",
    "\n",
    "\n",
    "\n",
    "2.2 Global/Derivative-Free Methods\n",
    "\n",
    "Underlying Principle: Explore search space without derivatives, using heuristics or sampling; slower but robust to poor initials or multimodality.\n",
    "Examples:\n",
    "\n",
    "Bracketing Methods: Bisection (scalar, interval-based), Brent's (hybrid secant/bisection).\n",
    "Population-Based: Genetic algorithms, particle swarm (evolutionary; for global roots).\n",
    "Trust-Region Dogleg: For unconstrained nonlinear equations.\n",
    "\n",
    "\n",
    "SciPy Availability:\n",
    "\n",
    "Available: scipy.optimize.root (with 'df-sane' for derivative-free spectral), scipy.optimize.brentq/brenth (scalar bracketing), scipy.optimize.minimize (can adapt for root-finding via optimization, e.g., 'powell' or 'nelder-mead').\n",
    "Not Available: Built-in evolutionary/global methods like differential evolution for pure root-finding (though scipy.optimize.differential_evolution exists for optimization); no interval arithmetic solvers (e.g., for guaranteed enclosures; use pyinterval)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3a71097d12af54c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "795b822204668219"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1. Linear Solvers\n",
    "These solve $\\mathbf{Ax} = \\mathbf{b}$ where $\\mathbf{A}$ is a matrix, $\\mathbf{x}$ is the unknown vector, and $\\mathbf{b}$ is a known vector. Principles include factorization (direct) or successive approximations (iterative), with considerations for matrix properties (e.g., sparse, symmetric, positive definite) to ensure stability and efficiency.\n",
    "###  1.1 Direct Methods\n",
    "\n",
    "Underlying Principle: Decompose the matrix into factors (e.g., triangular) for exact solution in finite steps, assuming no round-off errors. Efficient for dense matrices of moderate size; sensitive to conditioning and pivoting for numerical stability.\n",
    "Examples:\n",
    "\n",
    "#### 1.1.1 Gaussian Elimination / LU Decomposition:\n",
    "Factorizes $\\mathbf{A} = \\mathbf{LU}$ (lower/upper triangular), solves via forward/backward substitution. Partial pivoting (PLU) improves stability.\n",
    "\n",
    "scipy.linalg.solve (general direct solver using LAPACK (Linear Algebra PACKage); handles LU internally), scipy.linalg.lu (explicit LU with pivoting),\n",
    "\n",
    "\n",
    "The SciPy command for this decomposition is linalg.lu. Such a decomposition is often useful for solving many simultaneous equations where the left-hand side does not change but the right-hand side does. For example, suppose we are going to solve\n",
    "\n",
    "\n",
    "\n",
    "### 1.1.2 Finding the inverse\n",
    "\n",
    "In SciPy, the matrix inverse of the NumPy array, A, is obtained using linalg.inv (A), or using A.I if A is a Matrix. For example, let\n",
    "\n",
    "The following example demonstrates this computation in SciPy\n",
    "\n",
    "#### 1.1.3 Cholesky Decomposition:\n",
    "For symmetric positive definite matrices, $\\mathbf{A} = \\mathbf{LL}^T$ or $\\mathbf{UU}^T$; faster than LU (about half the operations).\n",
    "\n",
    "#### 1.1.4 QR Decomposition:\n",
    "Orthogonal factorization $\\mathbf{A} = \\mathbf{QR}$; useful for least-squares or underdetermined systems.\n",
    "\n",
    "#### 1.1.5 Singular Value Decomposition (SVD):\n",
    "[VERY IMPORTANT ALGORITHM!]\n",
    "\n",
    "$\\mathbf{A} = \\mathbf{U\\Sigma V}^T$; handles ill-conditioned or rectangular matrices, provides pseudo-inverse.\n",
    "\n",
    "#### 1.1.6  band-matrix solvers\n",
    "\n",
    "Available: scipy.linalg.solve (general direct solver using LAPACK; handles LU internally), scipy.linalg.lu (explicit LU with pivoting), scipy.linalg.cholesky, scipy.linalg.qr, scipy.linalg.svd.\n",
    "Not Available: Built-in band-matrix solvers (though scipy.linalg.solve_banded exists for banded systems); no native parallel direct solvers (use external libraries like PETSc).\n",
    "\n",
    "\n",
    "### 1.1.6 Condition exception handling\n",
    "\n",
    "ill-conditioning (check with numpy.linalg.cond)\n",
    "\n",
    "### 1.2 Iterative Methods\n",
    "\n",
    "Underlying Principle: Start with an initial guess and refine iteratively until convergence (e.g., residual below tolerance). Ideal for large sparse systems; convergence depends on preconditioning and matrix spectrum.\n",
    "Examples:\n",
    "\n",
    "### 1.2.1 Stationary Methods: Jacobi (diagonal dominance),\n",
    "### 1.2.2 Gauss-Seidel (updates in-place),\n",
    "### 1.2.3  Successive Over-Relaxation (SOR; accelerates with relaxation parameter).\n",
    "### 1.2.4 Krylov Subspace Methods: Conjugate Gradient (CG; for symmetric positive definite),\n",
    "### 1.2.5 Generalized Minimal Residual (GMRES; for nonsymmetric),\n",
    "### 1.2.6 Biconjugate Gradient Stabilized (BiCGSTAB).\n",
    "### 1.2.7 Multigrid Methods: Hierarchical smoothing and coarsening for elliptic PDEs (e.g., geometric/algebraic multigrid).\n",
    "### 1.2.8 Jacobi/Gauss-Seidel/SOR\n",
    "(implement manually or use external like PyAMG for multigrid); no native distributed-memory iterative solvers (e.g., for HPC; use PETSc or hypre).\n",
    "\n",
    "Available (for sparse matrices): scipy.sparse.linalg.cg, scipy.sparse.linalg.gmres, scipy.sparse.linalg.bicgstab, scipy.sparse.linalg.minres (Minimum Residual for symmetric indefinite).\n",
    "Preconditioners via scipy.sparse.linalg.spilu (incomplete LU).\n",
    "Not Available: Built-in Jacobi/Gauss-Seidel/SOR (implement manually or use external like PyAMG for multigrid); no native distributed-memory iterative solvers (e.g., for HPC; use PETSc or hypre)."
   ],
   "id": "579ec97f7c3a3699"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T19:16:22.034859Z",
     "start_time": "2025-10-30T19:16:21.741548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "# Example matrix (slightly modified to avoid singularity)\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 10]])\n",
    "\n",
    "# Compute SVD: Returns U, singular values S (as vector), and Vt (V transpose)\n",
    "U, S, Vt = linalg.svd(A)\n",
    "\n",
    "# Display results\n",
    "print(\"U matrix:\")\n",
    "print(U)\n",
    "print(\"\\nSingular values:\")\n",
    "print(S)\n",
    "print(\"\\nVt matrix:\")\n",
    "print(Vt)\n",
    "\n",
    "# Reconstruct A to verify (U * diag(S) * Vt)\n",
    "reconstructed_A = np.dot(U, np.dot(np.diag(S), Vt))\n",
    "print(\"\\nReconstructed A:\")\n",
    "print(np.round(reconstructed_A, decimals=10))  # Round for floating-point precision"
   ],
   "id": "b159906c813ec4f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U matrix:\n",
      "[[-0.20933734  0.96438514  0.16167618]\n",
      " [-0.50384851  0.03532145 -0.86306956]\n",
      " [-0.8380421  -0.26213299  0.47850992]]\n",
      "\n",
      "Singular values:\n",
      "[17.41250517  0.87516135  0.19686652]\n",
      "\n",
      "Vt matrix:\n",
      "[[-0.46466755 -0.55375455 -0.69097031]\n",
      " [-0.83328635  0.00949949  0.55275999]\n",
      " [ 0.2995295  -0.83262576  0.46585022]]\n",
      "\n",
      "Reconstructed A:\n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8. 10.]]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In practice, for solving systems, you might use the pseudo-inverse from SVD for least-squares: pinv_A = np.dot(Vt.T, np.dot(np.diag(1/S), U.T))",
   "id": "4d87f3df27cbecba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T19:17:52.611483Z",
     "start_time": "2025-10-30T19:17:52.609764Z"
    }
   },
   "cell_type": "code",
   "source": "pinv_A = np.dot(Vt.T, np.dot(np.diag(1/S), U.T))",
   "id": "b8d0bc6d47f212bc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ce953b27a48e05d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e40a7d690b634182"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ced31c05c46b2a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c81ec247781c3483"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. Special-Purpose Solvers\n",
    "These handle structured problems beyond standard linear/nonlinear, often integrating with the above.\n",
    "3.1 Eigenvalue Solvers\n",
    "\n",
    "Underlying Principle: Find $\\lambda, \\mathbf{v}$ such that $\\mathbf{A v} = \\lambda \\mathbf{v}$; uses iterations like power method or QR algorithm.\n",
    "Examples: Standard eigenvalue, generalized ($\\mathbf{A v} = \\lambda \\mathbf{B v}$).\n",
    "SciPy Availability: Available (scipy.linalg.eig, scipy.linalg.eigh for Hermitian, scipy.sparse.linalg.eigs for sparse).\n",
    "\n",
    "3.2 Least-Squares Solvers\n",
    "\n",
    "Underlying Principle: Minimize $\\|\\mathbf{Ax} - \\mathbf{b}\\|^2$; uses SVD or normal equations.\n",
    "Examples: Linear (LSQR), nonlinear (curve_fit).\n",
    "SciPy Availability: Available (scipy.linalg.lstsq, scipy.optimize.least_squares, scipy.optimize.curve_fit).\n",
    "\n",
    "3.3 Sparse and Large-Scale Extensions\n",
    "\n",
    "Underlying Principle: Exploit sparsity to reduce storage/computation; iterative with preconditioners.\n",
    "SciPy Availability: Strong support via scipy.sparse and scipy.sparse.linalg; missing: Advanced algebraic multigrid (use PyAMG).\n",
    "\n",
    "Recommendations for Course Integration\n",
    "\n",
    "In Notebooks: Demonstrate with Python examples, e.g., using scipy.linalg.solve for linear systems in Unit 4 (Simulation I â€“ algebraic systems). Include caveats like ill-conditioning (check with numpy.linalg.cond).\n",
    "What SciPy Excels At: Dense linear algebra and basic nonlinear root-finding; efficient for engineering-scale problems (<10^4 variables).\n",
    "Limitations and Alternatives: For very large/sparse systems, suggest PETSc or SuiteSparse; for symbolic Jacobians, integrate SymPy (available in the environment). Avoid over-reliance on SciPy for production HPC."
   ],
   "id": "5db03b2474d47b0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5507ae79f6f82ca0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
